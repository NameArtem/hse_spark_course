{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "younger-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.feature import RFormula, Binarizer\n",
    "from pyspark.ml.feature import QuantileDiscretizer, Bucketizer\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import  StringType, DoubleType, IntegerType, ArrayType, StringType\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confidential-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-all\").getOrCreate()\n",
    "\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-application",
   "metadata": {},
   "source": [
    "#  Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integral-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "                                    (0, \"Python python Spark Spark\"),\n",
    "                                    (1, \"Python SQL\")],\n",
    "                                 [\"document\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indian-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "harmful-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|vocabList|counts|\n",
      "+---------+------+\n",
      "|   python|   3.0|\n",
      "|    spark|   2.0|\n",
      "|      sql|   1.0|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_counts = model.transform(sentenceData)\\\n",
    "                    .select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "vocabList = model.stages[1].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spiritual-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(3, {0: 2.0, 1: 2.0})),\n",
       " Row(rawFeatures=SparseVector(3, {0: 1.0, 2: 1.0}))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = model.transform(sentenceData).select('rawFeatures').collect()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hydraulic-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures        |features                          |\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(3,[0,1],[2.0,2.0])|(3,[0,1],[0.0,0.8109302162163288])|\n",
      "|1       |Python SQL               |[python, sql]                 |(3,[0,2],[1.0,1.0])|(3,[0,2],[0.0,0.4054651081081644])|\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "animated-senate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'spark', 'sql']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "vectorizerModel = model.stages[1]\n",
    "vocabList = vectorizerModel.vocabulary\n",
    "vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "noted-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        rawFeatures|\n",
      "+-------------------+\n",
      "|(3,[0,1],[2.0,2.0])|\n",
      "|(3,[0,2],[1.0,1.0])|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawFeatures = model.transform(sentenceData).select('rawFeatures')\n",
    "rawFeatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "binary-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------------+---------------+\n",
      "|        rawFeatures|indices|         values|          Terms|\n",
      "+-------------------+-------+---------------+---------------+\n",
      "|(3,[0,1],[2.0,2.0])| [0, 1]|[2.0, 2.0, 0.0]|[python, spark]|\n",
      "|(3,[0,2],[1.0,1.0])| [0, 2]|[1.0, 0.0, 1.0]|  [python, sql]|\n",
      "+-------------------+-------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices_udf = udf(lambda vector: vector.indices.tolist(), ArrayType(IntegerType()))\n",
    "values_udf = udf(lambda vector: vector.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "\n",
    "rawFeatures.withColumn('indices', indices_udf(F.col('rawFeatures')))\\\n",
    "           .withColumn('values', values_udf(F.col('rawFeatures')))\\\n",
    "           .withColumn(\"Terms\", termsIdx2Term(vocabList)(\"indices\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-growing",
   "metadata": {},
   "source": [
    "# HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "small-lightning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures        |features                          |\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(5,[1,4],[2.0,2.0])|(5,[1,4],[0.8109302162163288,0.0])|\n",
      "|1       |Python SQL               |[python, sql]                 |(5,[2,4],[1.0,1.0])|(5,[2,4],[0.4054651081081644,0.0])|\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")],\n",
    " [\"document\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "\n",
    "model = pipeline.fit(sentenceData)\n",
    "model.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-population",
   "metadata": {},
   "source": [
    "#  FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "convinced-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|features                                                |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\"),\n",
    "    (3.3, False, \"2\", \"bar\"),\n",
    "    (4.4, False, \"3\", \"baz\"),\n",
    "    (5.5, False, \"4\", \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-payment",
   "metadata": {},
   "source": [
    "# RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "apart-nurse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  features|label|\n",
      "+----------+-----+\n",
      "|[0.0,18.0]|  1.0|\n",
      "|[1.0,12.0]|  0.0|\n",
      "|[1.0,15.0]|  0.0|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"CA\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-entrepreneur",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "short-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-boulder",
   "metadata": {},
   "source": [
    "# Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "informed-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "|  3|    0.5|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2),\n",
    "    (3,0.5)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-warren",
   "metadata": {},
   "source": [
    "# Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interstate-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| age|\n",
      "+---+----+\n",
      "|  0|18.0|\n",
      "|  1|19.0|\n",
      "|  2| 8.0|\n",
      "|  3| 5.0|\n",
      "|  4| 2.0|\n",
      "+---+----+\n",
      "\n",
      "None\n",
      "+---+----+------+\n",
      "| id| age|result|\n",
      "+---+----+------+\n",
      "|  0|18.0|   2.0|\n",
      "|  1|19.0|   2.0|\n",
      "|  2| 8.0|   1.0|\n",
      "|  3| 5.0|   1.0|\n",
      "|  4| 2.0|   0.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"age\"])\n",
    "print(df.show())\n",
    "\n",
    "splits = [-float(\"inf\"),3, 10,float(\"inf\")]\n",
    "result_bucketizer = Bucketizer(splits=splits, inputCol=\"age\",outputCol=\"result\").transform(df)\n",
    "result_bucketizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "advanced-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| age|\n",
      "+---+----+\n",
      "|  0|18.0|\n",
      "|  1|19.0|\n",
      "|  2| 8.0|\n",
      "|  3| 5.0|\n",
      "|  4| 2.0|\n",
      "+---+----+\n",
      "\n",
      "None\n",
      "+---+----+-------+\n",
      "| id| age|buckets|\n",
      "+---+----+-------+\n",
      "|  0|18.0|    4.0|\n",
      "|  1|19.0|    4.0|\n",
      "|  2| 8.0|    3.0|\n",
      "|  3| 5.0|    2.0|\n",
      "|  4| 2.0|    1.0|\n",
      "+---+----+-------+\n",
      "\n",
      "+---+----+-------+\n",
      "| id| age|buckets|\n",
      "+---+----+-------+\n",
      "|  0|18.0|    4.0|\n",
      "|  1|19.0|    4.0|\n",
      "|  2| 8.0|    3.0|\n",
      "|  3| 5.0|    2.0|\n",
      "|  4| 2.0|    1.0|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QuantileDiscretizer\n",
    "data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"age\"])\n",
    "print(df.show())\n",
    "\n",
    "qds = QuantileDiscretizer(numBuckets=5, inputCol=\"age\", outputCol=\"buckets\",\n",
    "                               relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = qds.fit(df)\n",
    "bucketizer.transform(df).show()\n",
    "bucketizer.setHandleInvalid(\"skip\").transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-thickness",
   "metadata": {},
   "source": [
    "# labelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "immediate-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'label' to indexed column 'labelIndex'\n",
      "+---+-----+----------+\n",
      "| id|label|labelIndex|\n",
      "+---+-----+----------+\n",
      "|  0|  Yes|       1.0|\n",
      "|  1|  Yes|       1.0|\n",
      "|  2|  Yes|       1.0|\n",
      "|  3|   No|       0.0|\n",
      "|  4|   No|       0.0|\n",
      "|  5|   No|       0.0|\n",
      "+---+-----+----------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata\n",
      "\n",
      "Transformed indexed column 'labelIndex' back to original string column 'originalLabel' using labels in metadata\n",
      "+---+----------+-------------+\n",
      "| id|labelIndex|originalLabel|\n",
      "+---+----------+-------------+\n",
      "|  0|       1.0|          Yes|\n",
      "|  1|       1.0|          Yes|\n",
      "|  2|       1.0|          Yes|\n",
      "|  3|       0.0|           No|\n",
      "|  4|       0.0|           No|\n",
      "|  5|       0.0|           No|\n",
      "+---+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, \"Yes\"), (1, \"Yes\"), (2, \"Yes\"), (3, \"No\"), (4, \"No\"), (5, \"No\")],\n",
    "    [\"id\", \"label\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"originalLabel\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"labelIndex\", \"originalLabel\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-person",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "medieval-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_type = 'Normal'\n",
    "if scaler_type=='Normal':\n",
    "    scaler = Normalizer(inputCol=\"features\", outputCol=\"scaledFeatures\", p=1.0)\n",
    "elif scaler_type=='Standard':\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                            withStd=True, withMean=False)\n",
    "elif scaler_type=='MinMaxScaler':\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "elif scaler_type=='MaxAbsScaler':\n",
    "    scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "intermediate-blend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]|\n",
      "|  2|[4.0,10.0,2.0]|\n",
      "+---+--------------+\n",
      "\n",
      "+---+--------------+------------------+\n",
      "| id|      features|    scaledFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "df.show()\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[scaler])\n",
    "\n",
    "model  =pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "partial-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n",
      "Normalized using L^inf norm\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizer\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "substantial-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |scaledFeatures                                              |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.5,-1.0]|[0.6546536707079771,0.09352195295828246,-0.6546536707079772]|\n",
      "|1  |[2.0,1.0,1.0] |[1.3093073414159542,0.18704390591656492,0.6546536707079772] |\n",
      "|2  |[4.0,10.0,2.0]|[2.6186146828319083,1.8704390591656492,1.3093073414159544]  |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                            withStd=True, withMean=False)\n",
    "scaleredData = scaler.fit((dataFrame)).transform(dataFrame)\n",
    "scaleredData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-plenty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
