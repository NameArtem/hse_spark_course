{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2ad6e43-905b-46f1-b44e-e49ba2e99d66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels\n",
    "#!pip install PyArrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"best_one\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "199566c7-994f-44ed-8e24-f907fe437709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|group_id|sex|  x|  y|\n",
      "+--------+---+---+---+\n",
      "|       1|  M|  0|  2|\n",
      "|       1|  N|  1|  1|\n",
      "|       1|  M|  2|  0|\n",
      "|       1|  N|  0|  0|\n",
      "|       1|  F|  1|  0|\n",
      "+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# простой DF\n",
    "pdf = pd.DataFrame({'group_id':[1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4],\n",
    "                    'sex':['M','N','M','N','F','F','M','F','N','M','M','N','F','M','F','F','N','M','F','M','F','N','M','F','M','F'],\n",
    "                    'x':[0,1,2,0,1,5,2,3,4,5,6,2,3,4,1,2,6,7,8,5,3,4,1,7,6,5],\n",
    "                    'y':[2,1,0,0,0,5,2,5,3,4,5,6,1,2,5,6,7,8,9,4,2,5,8,10,5,6]})\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# ещё один способ визуализации\n",
    "display(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43309420-d124-440e-81e4-c86dad40a120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# схема данного DF\n",
    "result_schema =StructType([\n",
    "  StructField('group_id',DoubleType()),\n",
    "  StructField('sex',StringType()),\n",
    "  StructField('x',DoubleType())\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7938afaa-1cae-4cde-8e37-63c4c8d4dcbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------------+\n",
      "|group_id|sex|                  x|\n",
      "+--------+---+-------------------+\n",
      "|     4.0|  N|               null|\n",
      "|     2.0|  M| 0.7307692307692304|\n",
      "|     2.0|  N|               -1.5|\n",
      "|     3.0|  F| 0.6379310344827586|\n",
      "|     4.0|  F| 2.0000000000000004|\n",
      "|     3.0|  N|               null|\n",
      "|     4.0|  M|-0.5999999999999998|\n",
      "|     1.0|  M|-0.9999999999999998|\n",
      "|     1.0|  F| 1.2500000000000002|\n",
      "|     1.0|  N| 1.0000000000000004|\n",
      "|     2.0|  F|               null|\n",
      "|     3.0|  M| 2.0000000000000013|\n",
      "+--------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# немного магии\n",
    "# применим sklearn функции и обработки из pandas в Spark\n",
    "\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def ols(df):\n",
    "    group_id = df['group_id'].iloc[0]\n",
    "    sex = df['sex'].iloc[0]\n",
    "\n",
    "    if len(df) == 1:\n",
    "        return pd.DataFrame([[group_id] + [sex] + [None]], columns=['group_id'] + ['sex'] + ['x'])\n",
    "\n",
    "    else:        \n",
    "        y = df['y'].astype(int)\n",
    "        X = df['x'].astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        return pd.DataFrame([[group_id] + [sex] + [model.params[1]]], columns=['group_id'] + ['sex'] + ['x'])\n",
    "\n",
    "\n",
    "# применяем, как обчную функцию (а это ещё Spark!)\n",
    "df.groupby('group_id', 'sex').apply(ols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWM из задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|group_id|  x|    x_ewm|\n",
      "+--------+---+---------+\n",
      "|     1.0|0.0|      0.0|\n",
      "|     1.0|1.0|     0.75|\n",
      "|     1.0|2.0|1.6153846|\n",
      "|     1.0|0.0|    0.525|\n",
      "|     1.0|1.0|0.8429752|\n",
      "|     1.0|5.0|3.6181319|\n",
      "|     3.0|3.0|      3.0|\n",
      "|     3.0|4.0|     3.75|\n",
      "|     3.0|1.0|1.8461539|\n",
      "|     3.0|2.0|     1.95|\n",
      "|     3.0|6.0| 4.661157|\n",
      "|     3.0|7.0|6.2225275|\n",
      "|     3.0|8.0| 7.408051|\n",
      "|     3.0|5.0| 5.802439|\n",
      "|     2.0|2.0|      2.0|\n",
      "|     2.0|3.0|     2.75|\n",
      "|     2.0|4.0|3.6153846|\n",
      "|     2.0|5.0|     4.55|\n",
      "|     2.0|6.0|5.5206614|\n",
      "|     2.0|2.0|3.1703296|\n",
      "+--------+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions, types\n",
    "# можно применить функции из statsmodels\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "# создаем структуру, которая будет возвращена в Spark DataFrame\n",
    "schema = types.StructType([\n",
    "    types.StructField(\"group_id\", types.DoubleType(), True),\n",
    "    types.StructField(\"x\", types.FloatType(), True),\n",
    "    types.StructField(\"x_ewm\", types.FloatType(), True)\n",
    "])\n",
    "\n",
    "# функция UDF (принимает схему и группированный объект из Spark)\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def my_ema(pdf):\n",
    "    \n",
    "    # работаем с Pandas DataFrame\n",
    "    # применяем функцию \n",
    "    ewm_col = pdf['x'].ewm(com=0.5).mean()\n",
    "    \n",
    "    # создаем финальный DF\n",
    "    output = pd.DataFrame({'group_id': pdf['group_id'], 'x': pdf['x'], 'x_ewm': ewm_col})\n",
    "    return output\n",
    "\n",
    "\n",
    "df.groupby('group_id').apply(my_ema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "689e7248-1091-41ec-84ce-2b8e423528da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# На примере RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим данные и целевую\n",
    "TITANIC_URL = \"https://raw.githubusercontent.com/amueller/scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv\"\n",
    "TARGET = \"fare\"\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"sibsp\",\n",
    "    \"parch\",\n",
    "    \"age\"\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"sex\",\n",
    "    \"cabin\"\n",
    "]\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим данные\n",
    "df = (\n",
    "    pd.read_csv(TITANIC_URL)[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + [TARGET]]\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "for num_feat in NUMERICAL_FEATURES:\n",
    "    df[num_feat] = df[num_feat].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cabin</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>B5</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>male</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>male</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sibsp  parch      age     sex    cabin      fare\n",
       "0    0.0    0.0  29.0000  female       B5  211.3375\n",
       "1    1.0    2.0   0.9167    male  C22 C26  151.5500\n",
       "2    1.0    2.0   2.0000  female  C22 C26  151.5500\n",
       "3    1.0    2.0  30.0000    male  C22 C26  151.5500\n",
       "4    1.0    2.0  25.0000  female  C22 C26  151.5500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+-------+--------+\n",
      "|sibsp|parch|   age|   sex|  cabin|    fare|\n",
      "+-----+-----+------+------+-------+--------+\n",
      "|  0.0|  0.0|  29.0|female|     B5|211.3375|\n",
      "|  1.0|  2.0|0.9167|  male|C22 C26|  151.55|\n",
      "|  1.0|  2.0|   2.0|female|C22 C26|  151.55|\n",
      "|  1.0|  2.0|  30.0|  male|C22 C26|  151.55|\n",
      "|  1.0|  2.0|  25.0|female|C22 C26|  151.55|\n",
      "+-----+-----+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf = spark.createDataFrame(df)\n",
    "ddf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим RF из Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas UDF\n",
    "\n",
    "def spark_predict(model, cols) -> pyspark.sql.column:\n",
    "    \"\"\"\n",
    "        model: модель из Sklearn\n",
    "        cols (list-like): параметры для предикта\n",
    "    \"\"\"\n",
    "    @sf.pandas_udf(returnType=DoubleType())\n",
    "    def predict_pandas_udf(*cols):\n",
    "        X = pd.concat(cols, axis=1)\n",
    "        return pd.Series(model.predict(X))\n",
    "    \n",
    "    return predict_pandas_udf(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf\n",
    "rf = RandomForestRegressor()\n",
    "rf = rf.fit(df[NUMERICAL_FEATURES], df[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119.84782992, 141.522249  , 140.452749  , 129.287917  ,\n",
       "       129.908749  ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(df[NUMERICAL_FEATURES])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------+------------------+\n",
      "|sibsp|parch|   age|    fare|        prediction|\n",
      "+-----+-----+------+--------+------------------+\n",
      "|  0.0|  0.0|  29.0|211.3375| 119.8478299242064|\n",
      "|  1.0|  2.0|0.9167|  151.55|141.52224899999982|\n",
      "|  1.0|  2.0|   2.0|  151.55| 140.4527489999998|\n",
      "|  1.0|  2.0|  30.0|  151.55|129.28791699999985|\n",
      "|  1.0|  2.0|  25.0|  151.55| 129.9087489999999|\n",
      "+-----+-----+------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColum(str, функция (модель, фичи))\n",
    "# !!!!!!!!!!!!!!!!!11\n",
    "(\n",
    "    ddf\n",
    "    .select(NUMERICAL_FEATURES + [TARGET])\n",
    "    .withColumn(\"prediction\", spark_predict(rf, NUMERICAL_FEATURES).alias(\"prediction\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В виде pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------+------------------+\n",
      "|sibsp|parch|   age|    fare|      pipe_predict|\n",
      "+-----+-----+------+--------+------------------+\n",
      "|  0.0|  0.0|  29.0|211.3375|110.63214057961764|\n",
      "|  1.0|  2.0|0.9167|  151.55|144.46233199999978|\n",
      "|  1.0|  2.0|   2.0|  151.55|141.28503999999978|\n",
      "|  1.0|  2.0|  30.0|  151.55|135.22670799999986|\n",
      "|  1.0|  2.0|  25.0|  151.55|124.56416599999991|\n",
      "+-----+-----+------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[(\"scaler\", MinMaxScaler()), (\"predictor\", RandomForestRegressor())])\n",
    "pipe = pipe.fit(df[NUMERICAL_FEATURES], df[TARGET])\n",
    "\n",
    "(\n",
    "    ddf\n",
    "    .select(NUMERICAL_FEATURES + [TARGET])\n",
    "    .withColumn(\"pipe_predict\", spark_predict(pipe, NUMERICAL_FEATURES).alias(\"prediction\")).show(5)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "test_group_regression",
   "notebookOrigID": 3222454211517253,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
