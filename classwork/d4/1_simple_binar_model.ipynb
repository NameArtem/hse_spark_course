{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark-ML\").getOrCreate()\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|spam|message                                                                                                                                                    |\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|ham |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|spam|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "|ham |U dun say so early hor... U c already then say...                                                                                                          |\n",
      "|ham |Nah I don't think he goes to usf, he lives around here though                                                                                              |\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read (Spark Session)\n",
    "location = \"data/SMSSpamCollection\"\n",
    "raw_df = spark.read.option(\"delimiter\", \"\\t\").csv(location).toDF(\"spam\", \"message\")\n",
    "\n",
    "raw_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|spam|message                                                                                                                                                    |\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|ham |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|spam|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "|ham |U dun say so early hor... U c already then say...                                                                                                          |\n",
      "|ham |Nah I don't think he goes to usf, he lives around here though                                                                                              |\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read (Spark Contex | RDD)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "raw = sparkContext.textFile(location) \\\n",
    "                  .map(lambda line: line.split(\"\\t\")) \\\n",
    "                  .map(lambda row: Row(spam=row[0].strip(), message=row[1].strip()))\n",
    "\n",
    "raw_df = spark.createDataFrame(raw)\n",
    "raw_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все слова из SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "# transformed = tokenizer.transform(raw_df)\n",
    "# transformed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалим стоп-слова )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "# cleaned = remover.transform(transformed)\n",
    "# cleaned.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so', 'than', 'too', 'very', 's']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWordsRemover().getStopWords()[115:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим в стоп-слово ‘-‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# повторим действие с расширенным списком\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание из текст SMS в фичи для CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация фичей (для модели)\n",
    "count_vect_model = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(cleaned)\n",
    "featured = count_vect_model.transform(cleaned)\n",
    "\n",
    "# конвертация спам / не спам в бинарный признак (а как правильно делать OHE? Какие варианты вы знаете?)\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "# indexed = indexer.transform(featured)\n",
    "# indexed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделяем на train / test\n",
    "\n",
    "А какие способы разделения Вы ещё знаете?\n",
    "\n",
    "Какая между ними разница?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham|\"ALRITE HUNNY!WOT...|[\"alrite, hunny!w...|[\"alrite, hunny!w...|(13463,[0,2,65,20...|  0.0|\n",
      "| ham|\"CAN I PLEASE COM...|[\"can, i, please,...|[\"can, please, co...|(13463,[8,14,42,6...|  0.0|\n",
      "| ham|\"Getting tickets ...|[\"getting, ticket...|[\"getting, ticket...|(13463,[2,8,18,30...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = indexed.randomSplit([0.7, 0.3], seed = 28)\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Почему этот вариант?\n",
    "\n",
    "Как выглядит ЛогРег?\n",
    "\n",
    "Что ещё можно с помощью этого алгоритма(фокус на кривую) измерять (или он только для бинарной классификации)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|spam|             message|               words|            filtered|            features|label|       rawPrediction|         probability|prediction|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| ham|\"AH POOR BABY!HOP...|[\"ah, poor, baby!...|[\"ah, poor, baby!...|(13463,[0,2,7,66,...|  0.0|[1.85777096161441...|[0.86503692386045...|       0.0|\n",
      "| ham|\"Are you comingdo...|[\"are, you, comin...|[\"are, comingdown...|(13463,[3585,1043...|  0.0|[1.85777096161441...|[0.86503692386045...|       0.0|\n",
      "| ham|\"Aww you must be ...|[\"aww, you, must,...|[\"aww, must, near...|(13463,[301,2849,...|  0.0|[1.85777096161441...|[0.86503692386045...|       0.0|\n",
      "| ham|\"BOO BABE! U ENJO...|[\"boo, babe!, u, ...|[\"boo, babe!, u, ...|(13463,[0,2,44,67...|  0.0|[1.85777096161441...|[0.86503692386045...|       0.0|\n",
      "| ham|\"EY! CALM DOWNON ...|[\"ey!, calm, down...|[\"ey!, calm, down...|(13463,[0,4,12,27...|  0.0|[1.85777096161441...|[0.86503692386045...|       0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# установка параметров\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "# обучение\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# пре\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "predictions.show(5)\n",
    "\n",
    "# predictions.select(\"features\", \"label\", \"prediction\").show(5)\n",
    "\n",
    "# оценим качество\n",
    "# roc-auc - что это, как строится данная кривая?\n",
    "# evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"roc_auc \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# А чем отличается RF от Boosting над решающими деревьями?\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "# evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"roc_auc \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы из слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              ngrams|            features|           features2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[\"alrite hunny!wo...|(13463,[0,2,65,20...|(36628,[73,4554,6...|\n",
      "|[\"can please, ple...|(13463,[8,14,42,6...|(36628,[5279,6510...|\n",
      "|[\"getting tickets...|(13463,[2,8,18,30...|(36628,[121,1565,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим n-граммы из слов и мы добавми их в модель\n",
    "ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngrams_df = ngram.transform(indexed)\n",
    "\n",
    "# повторяем векторизацию\n",
    "count_vect_model = CountVectorizer().setInputCol(\"ngrams\").setOutputCol(\"features2\").fit(ngrams_df)\n",
    "featured = count_vect_model.transform(ngrams_df)\n",
    "\n",
    "train, test = featured.randomSplit([0.7, 0.3], seed = 28)\n",
    "train.select(\"ngrams\", \"features\", \"features2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features2\").setNumTrees(10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "# evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"roc_auc \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токены + удаляем стоп-слова\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "stopwords = StopWordsRemover().getStopWords()+ [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "\n",
    "# векторизируем\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "\n",
    "# индексируем\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "\n",
    "# создаем ЛогРег\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "#\n",
    "# Pipeline\n",
    "#\n",
    "# создание pipeline\n",
    "# почему мы делаем pipeline?\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, lr])\n",
    "\n",
    "# обучаем (запуск всех шагов выполнения с 1 по N)\n",
    "model = pipeline.fit(raw_df)\n",
    "\n",
    "# сохраняем результат\n",
    "# почему мы сохраняем весь pipeline, а не одну модель?\n",
    "model.write().overwrite().save(\"model/spam_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка\n",
    "pipeline = pipeline = PipelineModel.load(\"model/spam_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  0.5\n"
     ]
    }
   ],
   "source": [
    "# воспроизводим результат из сохранения\n",
    "lr_predictions = pipeline.transform(raw_df)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"roc_auc \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
