{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2ad6e43-905b-46f1-b44e-e49ba2e99d66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyArrow\n",
      "  Downloading pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.7 MB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from PyArrow) (1.20.2)\n",
      "Installing collected packages: PyArrow\n",
      "Successfully installed PyArrow-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install statsmodels\n",
    "#!pip install PyArrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"best_one\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "199566c7-994f-44ed-8e24-f907fe437709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|group_id|sex|  x|  y|\n",
      "+--------+---+---+---+\n",
      "|       1|  M|  0|  2|\n",
      "|       1|  N|  1|  1|\n",
      "|       1|  M|  2|  0|\n",
      "|       1|  N|  0|  0|\n",
      "|       1|  F|  1|  0|\n",
      "+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# простой DF\n",
    "pdf = pd.DataFrame({'group_id':[1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4],\n",
    "                    'sex':['M','N','M','N','F','F','M','F','N','M','M','N','F','M','F','F','N','M','F','M','F','N','M','F','M','F'],\n",
    "                    'x':[0,1,2,0,1,5,2,3,4,5,6,2,3,4,1,2,6,7,8,5,3,4,1,7,6,5],\n",
    "                    'y':[2,1,0,0,0,5,2,5,3,4,5,6,1,2,5,6,7,8,9,4,2,5,8,10,5,6]})\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# ещё один способ визуализации\n",
    "display(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43309420-d124-440e-81e4-c86dad40a120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# схема данного DF\n",
    "result_schema =StructType([\n",
    "  StructField('group_id',DoubleType()),\n",
    "  StructField('sex',StringType()),\n",
    "  StructField('x',DoubleType())\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7938afaa-1cae-4cde-8e37-63c4c8d4dcbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------------+\n",
      "|group_id|sex|                  x|\n",
      "+--------+---+-------------------+\n",
      "|     4.0|  N|               null|\n",
      "|     2.0|  M| 0.7307692307692304|\n",
      "|     2.0|  N|               -1.5|\n",
      "|     3.0|  F| 0.6379310344827586|\n",
      "|     4.0|  F| 2.0000000000000004|\n",
      "|     3.0|  N|               null|\n",
      "|     4.0|  M|-0.5999999999999998|\n",
      "|     1.0|  M|-0.9999999999999998|\n",
      "|     1.0|  F| 1.2500000000000002|\n",
      "|     1.0|  N| 1.0000000000000004|\n",
      "|     2.0|  F|               null|\n",
      "|     3.0|  M| 2.0000000000000013|\n",
      "+--------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# немного магии\n",
    "# применим sklearn функции и обработки из pandas в Spark\n",
    "\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def ols(df):\n",
    "    group_id = df['group_id'].iloc[0]\n",
    "    sex = df['sex'].iloc[0]\n",
    "\n",
    "    if len(df) == 1:\n",
    "        return pd.DataFrame([[group_id] + [sex] + [None]], columns=['group_id'] + ['sex'] + ['x'])\n",
    "\n",
    "    else:        \n",
    "        y = df['y'].astype(int)\n",
    "        X = df['x'].astype(int)\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        return pd.DataFrame([[group_id] + [sex] + [model.params[1]]], columns=['group_id'] + ['sex'] + ['x'])\n",
    "\n",
    "\n",
    "# применяем, как обчную функцию (а это ещё Spark!)\n",
    "df.groupby('group_id', 'sex').apply(ols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "689e7248-1091-41ec-84ce-2b8e423528da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# На примере RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим данные и целевую\n",
    "TITANIC_URL = \"https://raw.githubusercontent.com/amueller/scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv\"\n",
    "TARGET = \"fare\"\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"sibsp\",\n",
    "    \"parch\",\n",
    "    \"age\"\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"sex\",\n",
    "    \"cabin\"\n",
    "]\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим данные\n",
    "df = (\n",
    "    pd.read_csv(TITANIC_URL)[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + [TARGET]]\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "for num_feat in NUMERICAL_FEATURES:\n",
    "    df[num_feat] = df[num_feat].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cabin</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>B5</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>male</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>male</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>female</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sibsp  parch      age     sex    cabin      fare\n",
       "0    0.0    0.0  29.0000  female       B5  211.3375\n",
       "1    1.0    2.0   0.9167    male  C22 C26  151.5500\n",
       "2    1.0    2.0   2.0000  female  C22 C26  151.5500\n",
       "3    1.0    2.0  30.0000    male  C22 C26  151.5500\n",
       "4    1.0    2.0  25.0000  female  C22 C26  151.5500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+-------+--------+\n",
      "|sibsp|parch|   age|   sex|  cabin|    fare|\n",
      "+-----+-----+------+------+-------+--------+\n",
      "|  0.0|  0.0|  29.0|female|     B5|211.3375|\n",
      "|  1.0|  2.0|0.9167|  male|C22 C26|  151.55|\n",
      "|  1.0|  2.0|   2.0|female|C22 C26|  151.55|\n",
      "|  1.0|  2.0|  30.0|  male|C22 C26|  151.55|\n",
      "|  1.0|  2.0|  25.0|female|C22 C26|  151.55|\n",
      "+-----+-----+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf = spark.createDataFrame(df)\n",
    "ddf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим RF из Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas UDF\n",
    "\n",
    "def spark_predict(model, cols) -> pyspark.sql.column:\n",
    "    \"\"\"\n",
    "        model: модель из Sklearn\n",
    "        cols (list-like): параметры для предикта\n",
    "    \"\"\"\n",
    "    @sf.pandas_udf(returnType=DoubleType())\n",
    "    def predict_pandas_udf(*cols):\n",
    "        X = pd.concat(cols, axis=1)\n",
    "        return pd.Series(model.predict(X))\n",
    "    \n",
    "    return predict_pandas_udf(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf\n",
    "rf = RandomForestRegressor()\n",
    "rf = rf.fit(df[NUMERICAL_FEATURES], df[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119.84782992, 141.522249  , 140.452749  , 129.287917  ,\n",
       "       129.908749  ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(df[NUMERICAL_FEATURES])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------+------------------+\n",
      "|sibsp|parch|   age|    fare|        prediction|\n",
      "+-----+-----+------+--------+------------------+\n",
      "|  0.0|  0.0|  29.0|211.3375| 119.8478299242064|\n",
      "|  1.0|  2.0|0.9167|  151.55|141.52224899999982|\n",
      "|  1.0|  2.0|   2.0|  151.55| 140.4527489999998|\n",
      "|  1.0|  2.0|  30.0|  151.55|129.28791699999985|\n",
      "|  1.0|  2.0|  25.0|  151.55| 129.9087489999999|\n",
      "+-----+-----+------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColum(str, функция (модель, фичи))\n",
    "# !!!!!!!!!!!!!!!!!11\n",
    "(\n",
    "    ddf\n",
    "    .select(NUMERICAL_FEATURES + [TARGET])\n",
    "    .withColumn(\"prediction\", spark_predict(rf, NUMERICAL_FEATURES).alias(\"prediction\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В виде pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------+------------------+\n",
      "|sibsp|parch|   age|    fare|      pipe_predict|\n",
      "+-----+-----+------+--------+------------------+\n",
      "|  0.0|  0.0|  29.0|211.3375|110.63214057961764|\n",
      "|  1.0|  2.0|0.9167|  151.55|144.46233199999978|\n",
      "|  1.0|  2.0|   2.0|  151.55|141.28503999999978|\n",
      "|  1.0|  2.0|  30.0|  151.55|135.22670799999986|\n",
      "|  1.0|  2.0|  25.0|  151.55|124.56416599999991|\n",
      "+-----+-----+------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[(\"scaler\", MinMaxScaler()), (\"predictor\", RandomForestRegressor())])\n",
    "pipe = pipe.fit(df[NUMERICAL_FEATURES], df[TARGET])\n",
    "\n",
    "(\n",
    "    ddf\n",
    "    .select(NUMERICAL_FEATURES + [TARGET])\n",
    "    .withColumn(\"pipe_predict\", spark_predict(pipe, NUMERICAL_FEATURES).alias(\"prediction\")).show(5)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "test_group_regression",
   "notebookOrigID": 3222454211517253,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
