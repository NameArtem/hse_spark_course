{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "second-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"best_one\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "independent-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|     values|\n",
      "+-----------+\n",
      "|81.37291811|\n",
      "|25.70097086|\n",
      "|4.942646012|\n",
      "|43.02085256|\n",
      "|81.69058902|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = (spark.read\n",
    "                  .format('com.databricks.spark.csv')\n",
    "                  .option(\"header\", \"true\") \n",
    "                  .option(\"inferSchema\", \"true\") \n",
    "                  .load(\"skewdata.csv\")\n",
    "                  )\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recent-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "# не забывайте, если нет понимания, как сделать на spark - сделайте на sql\n",
    "#data_df.createOrReplaceTempView('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-detective",
   "metadata": {},
   "source": [
    "# confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ci функция\n",
    "def getConfidenceInterval(inputDataFrame,num_of_samples, left_quantile_fraction, right_quantile_fraction):\n",
    "    #симуляция построения сэмплов\n",
    "    sample_means = np.empty([num_of_samples])\n",
    "    \n",
    "    for n in range(0,num_of_samples):\n",
    "        # набор агрегатов - результатов\n",
    "        sample_means[n] = (inputDataFrame.sample(withReplacement = True, fraction=1.0)\n",
    "                   .selectExpr(\"avg(values) as mean\")\n",
    "                   .collect()[0]\n",
    "                   .asDict()\n",
    "                   .get('mean'))\n",
    "            \n",
    "    ## сортировка\n",
    "    sample_means.sort()\n",
    "    \n",
    "    ## \n",
    "    sampleMeans_local_df = pd.DataFrame(sample_means)\n",
    "    \n",
    "    ## из пандас в spark\n",
    "    fields = [StructField(\"mean_values\", DoubleType(), True)]\n",
    "    schema = StructType(fields)\n",
    "    sampleMeans_df = spark.createDataFrame(sampleMeans_local_df, schema)\n",
    "    \n",
    "    ## расчет квартилей ( 25 - 75 персентилей)\n",
    "    sampleMeans_df.createOrReplaceTempView('Guru_SampleMeansTable')\n",
    "    quantiles_df = spark.sql(\"select percentile(cast(mean_values as bigint),\"\n",
    "                                  \"array(\"+str(left_quantile_fraction)+\",\"+str(right_quantile_fraction)+\")) as \"\n",
    "                                  \"percentiles from Guru_SampleMeansTable\")\n",
    "    return quantiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "useful-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 95 % доверительынй интервал для данных\n",
    "quantiles_df = getConfidenceInterval(data_df, 1000, 0.025, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "authentic-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| percentiles|\n",
      "+------------+\n",
      "|[25.0, 38.0]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantiles_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-advance",
   "metadata": {},
   "source": [
    "# Clustered Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = (spark.read\n",
    "           .format('com.databricks.spark.csv')\n",
    "           .option(\"header\", \"true\") \n",
    "           .option(\"inferSchema\", \"true\") \n",
    "           .load(\"skewdata-policy-new.csv\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sustained-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|policyid|\n",
      "+--------+\n",
      "|       1|\n",
      "|       6|\n",
      "|       3|\n",
      "|       5|\n",
      "|       9|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyids = data_df.select('policyid').distinct()\n",
    "policyids.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laughing-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "## разделение сэмплов на фракции и их маркирование\n",
    "def clusteredSamples(data,policies,policyid_sample_fraction,num_of_samples):\n",
    "    \n",
    "    #пустой лист для нумерования\n",
    "    samples = []\n",
    "    \n",
    "    for n in range(0,num_of_samples):\n",
    "        \n",
    "        #создание сэмпла 1\n",
    "        policyids_sample = policies.sample(withReplacement=False, fraction=policyid_sample_fraction)\n",
    "        sample = policyids_sample.join(data,on='policyid',how='inner')\n",
    "        samples.append(sample)\n",
    "    \n",
    "    # список сэмплов\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "relative-amino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double],\n",
       " DataFrame[policyid: int, age: int, values: double]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleList = clusteredSamples(data_df,policyids,0.8,20)\n",
    "sampleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfied-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-3.4693]),\n",
       " DenseVector([-2.3257]),\n",
       " DenseVector([-2.6926]),\n",
       " DenseVector([-1.25]),\n",
       " DenseVector([-2.1493]),\n",
       " DenseVector([-2.7645]),\n",
       " DenseVector([-1.6272]),\n",
       " DenseVector([-2.1566]),\n",
       " DenseVector([-2.3443]),\n",
       " DenseVector([-2.5109]),\n",
       " DenseVector([-2.7963]),\n",
       " DenseVector([-2.5325]),\n",
       " DenseVector([-1.886]),\n",
       " DenseVector([-1.2457]),\n",
       " DenseVector([-3.2977]),\n",
       " DenseVector([-0.4748]),\n",
       " DenseVector([-2.6444]),\n",
       " DenseVector([-2.6921]),\n",
       " DenseVector([-2.3257]),\n",
       " DenseVector([-2.0651])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def runLinearRegression(samples):\n",
    "    # обучим регрессию на сэмплах\n",
    "    samples_coefficients = []\n",
    "    \n",
    "    #vector Assembler\n",
    "    feature_columns = ['age']\n",
    "    vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features_vector')\n",
    "    \n",
    "    #linear regresson model\n",
    "    lr = LinearRegression(featuresCol ='features_vector', \n",
    "                          labelCol = 'values',\n",
    "                          predictionCol = 'predicted_values',\n",
    "                          maxIter=5, \n",
    "                          elasticNetParam = 0.5,\n",
    "                          solver=\"l-bfgs\")\n",
    "    for i in range(0,len(samples)):\n",
    "        sample_df = samples[i]\n",
    "        sample_df1 = vectorAssembler.transform(sample_df)\n",
    "        \n",
    "        #Fit \n",
    "        sample_lr = lr.fit(sample_df1)\n",
    "        \n",
    "        #Save\n",
    "        samples_coefficients.append(sample_lr.coefficients)\n",
    "    \n",
    "    return samples_coefficients\n",
    "\n",
    "sampleCoefficients = runLinearRegression(sampleList)\n",
    "sampleCoefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-gravity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
