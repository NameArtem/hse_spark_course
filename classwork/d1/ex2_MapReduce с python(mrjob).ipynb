{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce на mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mrjob это библиотека для Python, которая позволяет создавать MapReduce jobs. \n",
    "\n",
    "Документация по MRJob: [https://mrjob.readthedocs.io/en/latest/](https://mrjob.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем файл из MapReduce(bash) `fruits.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r--r--r-- 1 user123 hadoopusers 429M Apr 17 23:00 file.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh fruits.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим файл `word_count.py` с помощью magic в Jupyter Notebook `%%file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "        \n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем определит кол-во мапперов и редьюсеров, на пример $10$ мапперов и $3$ редьюсеров. (игрушечный пример)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATAFILE=fruits.txt\n",
    "STREAMING_JAR=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "N=10\n",
    "\n",
    "# N map tasks\n",
    "python word_count.py\\\n",
    "    --jobconf mapreduce.job.maps=$N\\\n",
    "    --jobconf mapreduce.job.reduces=3\\\n",
    "    -r hadoop\\\n",
    "    --hadoop-streaming-jar $STREAMING_JAR\\\n",
    "    $DATAFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "START=$(date +%s);\n",
    "\n",
    "DATAFILE=# файл с большим набором данных\n",
    "STREAMING_JAR=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "N=4 # кол-во мапперов\n",
    "\n",
    "# N map tasks\n",
    "python word_count.py\\\n",
    "    --jobconf mapreduce.job.maps=$N\\\n",
    "    --jobconf mapreduce.job.reduces=3\\\n",
    "    -r hadoop --hadoop-streaming-jar $STREAMING_JAR\\\n",
    "    $DATAFILE\n",
    "    \n",
    "2>/dev/null\n",
    "\n",
    "END=$(date +%s);\n",
    "echo $((END-START)) | awk '{print \"Duration: \"int($1/60)\":\"int($1%60)}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
