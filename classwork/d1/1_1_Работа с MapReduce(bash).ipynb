{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание\n",
    "- [MapReduce?](#mapreduce)\n",
    "- [Mapper](#mapper)\n",
    "    - [Запускаем mapper](#testmapper)\n",
    "- [Hadoop](#hadoop)\n",
    "    - [Что такое Hadoop Streaming?](#hadoopstreaming)\n",
    "    - [Список директорий в Hadoop](#hdfs_ls)\n",
    "    - [Тестируем MapReduce на простом reducer](#dummyreducer)\n",
    "    - [Shuffling и sorting](#shuffling&sorting)\n",
    "- [Reducer](#reducer)\n",
    "    - [Запускаем reducer](#run)\n",
    "- [Запускаем mapreduce job с большими данными](#moredata)\n",
    "    - [Sort результат (`sort`)](#sortoutput)\n",
    "    - [Sort результат (в MapReduce)](#sortoutputMR)\n",
    "    - [Конфигурируем сортировку с `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n",
    "    - [Определяем конфигурацию опцией -D](#configuration_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce? <a name=\"mapreduce\"></a>\n",
    "\n",
    "MapReduce - это модель распределенных параллельных вычислений разработанная для процессов обработки больших объемов данных.\n",
    "\n",
    "Данные разделяются по специальным узлам(nodes), где работают процессы - mappers. Мапперы - это первый шаг обработки данных, они делают \"базовую обработку\" и передают результаты в reducer. Уже редьюсеры объединяют данные и создают финальный результат.\n",
    "\n",
    "![Map & Reduce](mapreduce.png)\n",
    "C [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) возможно использовать языки программирования для разработки mapper и reducer. Здесь будет описан способ использования Unix `bash`. ([Здесь](https://www.gnu.org/software/bash/manual/html_node/index.html) документация по bash).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper <a name=\"mapper\"></a>\n",
    "\n",
    "Давайте составим первый mapper скрипт `map.sh`. Mapper должен разбивать каждую строку на слова, добавлять число для счетчика и возвращать каждое слово отдельной строй, а через tab число для счетчика - 1.\n",
    "\n",
    "Пример: input \n",
    "<html>\n",
    "<pre>\n",
    "apple orange\n",
    "banana apple peach\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "`map.sh` результат:\n",
    "<html>\n",
    "<pre>\n",
    "apple   1\n",
    "orange  1\n",
    "banana  1\n",
    "apple  1\n",
    "peach  1\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "\n",
    "<a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) позволяют писать скрипты и выполнять команды Linux в Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile map.sh\n",
    "#!/bin/bash\n",
    "\n",
    "while read line\n",
    "do\n",
    " for word in $line \n",
    " do\n",
    "  if [ -n \"$word\" ] \n",
    "  then\n",
    "     echo -e ${word}\"\\t1\"\n",
    "  fi\n",
    " done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате вы получите файл `map.sh` в вашей текущей директории.\n",
    "\n",
    "**Note:** Каждый последущий запуск ячейки перезапишет файл `map.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -hl map.sh\n",
    "\n",
    "#dir - для Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем mapper <a name=\"testmapper\"></a>\n",
    "\n",
    "Чтобы запустить mapper, сначала создадим для его работы данные, создадим файл `fruits.txt` с набором фруктов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fruits.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile fruits.txt\n",
    "apple banana\n",
    "peach orange peach peach\n",
    "pineapple peach apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat fruits.txt\n",
    "\n",
    "# type - в Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним mapper. Используем для этого pipeline '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat fruits.txt|./map.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если скрипт `map.sh` не выполнится, то проверте права на его использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 700 map.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop <a name=\"hadoop\"></a>\n",
    "Теперь воспользуемся Hadoop и запустим наш скрипт с помощью Hadoop Streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
    "\n",
    "Hadoop Streaming - это библиотека в Hadoop, которая разработка для созданиях самописных мапперов и редьюсеров в исполняемые процессы MapReduce. \n",
    "\n",
    "\n",
    "Mapper и reducer читают данные из stdin и отправляют их в stdout. Обучно, колонки в данных разделяются с помощью `tab`. Если данные разделены другим разделителем, то надо будет определять разделитель. Для этого ознакомьтесь с `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) и [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
    "\n",
    "Пример MapReduce streaming синтаксиса:\n",
    "<html>\n",
    "<pre>\n",
    "    mapred streaming \\\n",
    "  -input myInputDirs \\\n",
    "  -output myOutputDir \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc\n",
    "\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "Документация для Hadoop Streaming от Apache Hadoop: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
    "\n",
    "Все настройки Hadoop Streaming и опции описаны здесь: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
      "Options:\n",
      "  -input          <path> DFS input file(s) for the Map step.\n",
      "  -output         <path> DFS output directory for the Reduce step.\n",
      "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
      "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
      "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
      "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
      "                  Deprecated. Use generic option \"-files\" instead.\n",
      "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
      "                  Optional. The input format class.\n",
      "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
      "                  Optional. The output format class.\n",
      "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
      "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
      "  -inputreader    <spec> Optional. Input recordreader spec.\n",
      "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
      "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
      "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
      "  -io             <identifier> Optional. Format to use for input to and output\n",
      "                  from mapper/reducer commands\n",
      "  -lazyOutput     Optional. Lazily create Output.\n",
      "  -background     Optional. Submit the job and don't wait till it completes.\n",
      "  -verbose        Optional. Print verbose output.\n",
      "  -info           Optional. Print detailed usage.\n",
      "  -help           Optional. Print help message.\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "\n",
      "For more details about these options:\n",
      "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"
     ]
    }
   ],
   "source": [
    "!mapred streaming --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список директорий в Hadoop <a name=\"hdfs_ls\"></a>\n",
    "\n",
    "Команда `hdfs dfs -l` покажет вам все, что находится в вашей домашней директории HDFS. \n",
    "\n",
    "`hdfs dfs` запускает файловую систему Hadoop. Списко всех достуных команды вы найдете в документации [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим  `wordcount` директорию с вложенной директорий `input` в Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируем fruits.txt в Hadoop директории `wordcount/input`.\n",
    "\n",
    "Почему мы это делаем? Файл `fruits.txt` должен располагаться в файловой системе Hadoop, а не локально. Когда файл находится в файловой системе Hadoop, то мы получаем возможность использовать фишки Hadoop: data partitioning, distributed processing, fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
    "hdfs dfs -mkdir wordcount/input\n",
    "hdfs dfs -put fruits.txt wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим.\n",
    "\n",
    "**Note:** Используйте опцию `-h` для `ls`, чтобы показать размер файла в `human-readable` форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 datalab supergroup         60 2019-11-18 08:49 wordcount/input/fruits.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h -R wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестируем MapReduce на простом reducer <a name=\"dummyreducer\"></a>\n",
    "\n",
    "Попроюуем запустить наш mapper используя простой reducer `/bin/cat`.\n",
    "\n",
    "**Warning:** mapreduce процесс всегда выводит большие output в командную строку, часто эта информация для нас бесполезна, нам нужен будет этот результат: <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n",
    "\n",
    "**Note:** Когда вы запускаете процесс, убедитесь, что финального файла нет в системе, иначе вы получите ошибку. Вы можете всегда добавлять такое действие: `hadoop fs -rmr wordcount/output 2>/dev/null`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -files map.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer /bin/cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:50 wordcount/output/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup         78 2019-11-18 08:50 wordcount/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если `output` содержит файл `_SUCCESS`, то ваш процесс завершился удачно\n",
    "\n",
    "**Note:** когда работает с большими данными, то к `cat` используйте `head` или `tail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\t1\n",
      "apple\t1\n",
      "banana\t1\n",
      "orange\t1\n",
      "peach\t1\n",
      "peach\t1\n",
      "peach\t1\n",
      "peach\t1\n",
      "pineapple\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part*|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n",
    "\n",
    "На изображении представлен процесс по результатам 2х мапперов. Процессы shuffle и sort происходят до попадания результатов в reducer\n",
    "\n",
    "![shuffle & sort](shuffle_sort.png)\n",
    "\n",
    "Shuffling и sorting являются самыми \"дорогими\" процессами в MapReduce.\n",
    "\n",
    "\n",
    "<b>Note:</b>  $2$ - базовое количество мапперов в Hadoop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer <a name=\"reducer\"></a>\n",
    "Напишем скрипт reducer `reduce.sh`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reduce.sh\n",
    "#!/bin/bash\n",
    "\n",
    "currkey=\"\"\n",
    "currcount=0\n",
    "while IFS=$'\\t' read -r key val\n",
    "do\n",
    "  if [[ $key == $currkey ]]\n",
    "  then\n",
    "      currcount=$(( currcount + val ))\n",
    "  else\n",
    "    if [ -n \"$currkey\" ]\n",
    "    then\n",
    "      echo -e ${currkey} \"\\t\" ${currcount} \n",
    "    fi\n",
    "    currkey=$key\n",
    "    currcount=1\n",
    "  fi\n",
    "done\n",
    "\n",
    "# stdout \n",
    "echo -e ${currkey} \"\\t\" ${currcount}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим правила для нашего reducer скрипта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 700 reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем reducer <a name=\"run\"></a>\n",
    "\n",
    "Выполним map и reduce без hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple \t 2\n",
      "banana \t 1\n",
      "orange \t 1\n",
      "peach \t 4\n",
      "pineapple \t 1\n"
     ]
    }
   ],
   "source": [
    "!cat fruits.txt|./map.sh|sort|./reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это хороший способ тестирования, если результаты правильные, то запустим на Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file map.sh \\\n",
    "  -file reduce.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим файл результата в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple \t 2\n",
      "banana \t 1\n",
      "orange \t 1\n",
      "peach \t 4\n",
      "pineapple \t 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part*|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запускаем mapreduce job с большими данными <a name=\"moredata\"></a>\n",
    "\n",
    "Создадим файл с данными, на основе реальной новостой статьи из интернета. Данных стало больше, представим, что это \"большие данные\".\n",
    "\n",
    "Для загрузки данных из интернета используйте, либо парсер Python, либо команду `wget` и удалите HTML командой `sed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n",
    "wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 17297\r",
      " 47% [....................................                                          ]  8192 / 17297\r",
      " 94% [.........................................................................     ] 16384 / 17297\r",
      "100% [..............................................................................] 17297 / 17297"
     ]
    }
   ],
   "source": [
    "# wget на Python\n",
    "import wget\n",
    "import lxml.html\n",
    "\n",
    "url = 'https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer'\n",
    "filename = wget.download(url)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    ff = f.read()\n",
    "    t = lxml.html.fromstring(ff)\n",
    "    result = t.text_content()\n",
    "    \n",
    "with open('sample_article.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1\n",
      "\t1\n",
      "\t1\n",
      "\t1\n",
      "Und\t1\n",
      "wo\t1\n",
      "warst\t1\n",
      "du\t1\n",
      "beim\t1\n",
      "Fall\t1\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим наш маппер для работы с пустыми строками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile map.sh\n",
    "#!/bin/bash\n",
    "\n",
    "while read line\n",
    "do\n",
    " for word in $line\n",
    " do\n",
    "  if [[ \"$line\" =~ [^[:space:]] ]]\n",
    "  then\n",
    "    if [ -n \"$word\" ]\n",
    "    then\n",
    "    echo -e ${word} \"\\t1\"\n",
    "    fi\n",
    "  fi\n",
    " done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und \t1\n",
      "wo \t1\n",
      "warst \t1\n",
      "du \t1\n",
      "beim \t1\n",
      "Fall \t1\n",
      "der \t1\n",
      "Mauer? \t1\n",
      "- \t1\n",
      " \t1\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map.sh` дает лучше результаты\n",
    "\n",
    "<b>Note:</b> при работе с реальными данными мы должны обращать внимание на их \"чистоту\" и добавлять в свой код процессы обработки (отчистки) данных.\n",
    "\n",
    "Запустим MapReduce на новых данных, но сначала загрузим их в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
    "hdfs dfs -put sample_article.txt wordcount/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 datalab supergroup      4.1 K 2019-11-18 08:50 wordcount/input\n"
     ]
    }
   ],
   "source": [
    "# проверим\n",
    "!hdfs dfs -ls -h wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверим reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und \t 1\n",
      "wo \t 1\n",
      "warst \t 1\n",
      "du \t 1\n",
      "beim \t 1\n",
      "Fall \t 1\n",
      "der \t 1\n",
      "Mauer? \t 1\n",
      "- \t 1\n",
      " \t 2\n"
     ]
    }
   ],
   "source": [
    "!cat sample_article.txt|./map.sh|./reduce.sh|head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop fs -rmr wordcount/output 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file map.sh \\\n",
    "  -file reduce.sh \\\n",
    "  -input wordcount/input \\\n",
    "  -output wordcount/output \\\n",
    "  -mapper map.sh \\\n",
    "  -reducer reduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:51 wordcount/output/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup       2273 2019-11-18 08:51 wordcount/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше данных - больше времени, так что этот процесс. Но учтите, что не всегда стоит использовать Hadoop, есть много кейсов, когда не стоит использовать сложные инструменты. Hadoop - только для больших инструментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&amp; \t 1\n",
      "(Herder-Verlag) \t 1\n",
      "- \t 1\n",
      "/ \t 2\n",
      "1950 \t 1\n",
      "24 \t 1\n",
      "30 \t 1\n",
      "30-Jährige \t 1\n",
      "<path \t 1\n",
      "AGB \t 1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part-00000|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort результат (`sort`) <a name=\"sortoutput\"></a>\n",
    "\n",
    "В результате мы получаем список из связки: слово - значение, нам нужно сделать сортировку по частоте употребления слова.\n",
    "\n",
    "Результат из reducer сортируется по ключам (словам) на основе результатов из mapper. Для получения сортированного результата используем Unix команду `sort` (с опциями `k2`, `n`, `r`, которое означают \"по полю 2\", \"числовое значение\", \"от большего к меньшему\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die \t 8\n",
      "der \t 6\n",
      "Cookies \t 4\n",
      "und \t 4\n",
      "derStandard.at \t 3\n",
      "Fall \t 3\n",
      "ich \t 3\n",
      "in \t 3\n",
      "kann \t 3\n",
      "ohne \t 3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort результат (в MapReduce) <a name=\"sortoutputMR\"></a>\n",
    "\n",
    "Если нам необходимо сделать сортировку в reducer, то мы можем применить простой трюк: создаем маппер, который будет менять местами слова (ключи) и их частоту (значение), на выходе маппера мы получим желанный эфект автоматически.\n",
    "\n",
    "Создадим новый маппер `swap_keyval.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing swap_keyval.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile swap_keyval.sh\n",
    "#!/bin/bash\n",
    "# скрипт меняет местами значения в строке\n",
    "# пример: \"word 100\" -> \"100 word\"\n",
    "\n",
    "while read key val\n",
    "do\n",
    " printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n",
    "done    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним наш новый маппер в старом пайплайне, не забудьте удалить `output_sorted`. \n",
    "\n",
    "Каждый шаг записывает свой результат на диск, что при больших объемах увеличивает время выполнения, это одна из причин появления Apache Spark [Apache Spark](https://spark.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r wordcount/output2 2>/dev/null\n",
    "mapred streaming \\\n",
    "  -file swap_keyval.sh \\\n",
    "  -input wordcount/output \\\n",
    "  -output wordcount/output2 \\\n",
    "  -mapper swap_keyval.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:52 wordcount/output2/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup       1945 2019-11-18 08:52 wordcount/output2/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tan\n",
      "1\t–\n",
      "1\tüberraschen.\n",
      "1\tÜber\n",
      "1\tzustimmungspflichtige\n",
      "1\tzustimmen\n",
      "1\tzum\n",
      "1\tzu.\n",
      "1\twiderrufen.\n",
      "1\twerden.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маппер сортирует от меньшего к большему (sort = ascending order). Самые частые слова будут внизу файла (смотрим хвост файла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\tkann\n",
      "3\tin\n",
      "3\tderStandard.at\n",
      "3\tich\n",
      "3\tFall\n",
      "3\tSie\n",
      "4\tCookies\n",
      "4\tund\n",
      "6\tder\n",
      "8\tdie\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конфигурируем сортировку с `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n",
    "\n",
    "Мы можем определить, как маппер будет сортировать свои результаты, для этого надо исользовать класс [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n",
    "<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n",
    "    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n",
    "    \n",
    "Данный класс (из библиотеке Hadoop) позволяет сделать похожие дополнения (опции) к сортировке, как Unix `sort`(`-n` - чиловая сортировка, `-r` от большего к меньшему, `-k pos1[,pos2]` сортировать по позиции элемента).\n",
    "\n",
    "Применим данный класс `KeyFieldBasedComparator` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
    "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
    "mapred streaming \\\n",
    "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
    "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "  -file swap_keyval.sh \\\n",
    "  -input wordcount/output \\\n",
    "  -output wordcount/output2 \\\n",
    "  -mapper swap_keyval.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 datalab supergroup          0 2019-11-18 08:52 wordcount/output2/_SUCCESS\n",
      "-rw-r--r--   3 datalab supergroup       1945 2019-11-18 08:52 wordcount/output2/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\tdie\n",
      "6\tder\n",
      "4\tund\n",
      "4\tCookies\n",
      "3\tFall\n",
      "3\tSie\n",
      "3\tich\n",
      "3\tkann\n",
      "3\twarst\n",
      "3\tin\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount/output2/part-00000|head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили необходимый результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем конфигурацию опцией -D<a name=\"configuration_variables\"></a>\n",
    "\n",
    "Опция `-D` позволяет перезаписать параметр в базовой конфигурации [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n",
    "(в документации [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n",
    "\n",
    "Иногда это требуется для исправления ошибки `out-of-memory` во время сортировки. Так как выделенной памяти может не хватать и её нужно увеличить, а если процесс выполняется и ресурсов очень много, то уменьшить. За это отвечает параметр `mapreduce.task.io.sort.mb`, он имеет размерность в Mb:\n",
    " <html>\n",
    "    <pre>-D mapreduce.task.io.sort.mb=512\n",
    "    </pre>\n",
    " </html>\n",
    "\n",
    " **Note:** `mapreduce.task.io.sort.mb` может быть не более  2047.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
