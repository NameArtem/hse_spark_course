{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%bash\n",
    "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
    "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
    "mapred streaming \\\n",
    "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
    "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "  -file swap_keyval.sh \\\n",
    "  -input wordcount/output \\\n",
    "  -output wordcount/output2 \\\n",
    "  -mapper swap_keyval.sh\n",
    "```\n",
    "\n",
    "\n",
    "### Что такое Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
    "\n",
    "Hadoop Streaming - это библиотека в Hadoop, которая разработка для созданиях самописных мапперов и редьюсеров в исполняемые процессы MapReduce. \n",
    "\n",
    "\n",
    "Mapper и reducer читают данные из stdin и отправляют их в stdout. Обучно, колонки в данных разделяются с помощью `tab`. Если данные разделены другим разделителем, то надо будет определять разделитель. Для этого ознакомьтесь с `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) и [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
    "\n",
    "Пример MapReduce streaming синтаксиса:\n",
    "<html>\n",
    "<pre>\n",
    "    mapred streaming \\\n",
    "  -input myInputDirs \\\n",
    "  -output myOutputDir \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc\n",
    "\n",
    "</pre>\n",
    "</html>\n",
    "\n",
    "Документация для Hadoop Streaming от Apache Hadoop: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
    "\n",
    "Все настройки Hadoop Streaming и опции описаны здесь: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce на mrjob\n",
    "\n",
    "\n",
    "\n",
    "mrjob это библиотека для Python, которая позволяет создавать MapReduce jobs. \n",
    "\n",
    "Документация по MRJob: [https://mrjob.readthedocs.io/en/latest/](https://mrjob.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем файл из MapReduce(bash) `fruits.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r--r--r-- 1 user123 hadoopusers 429M Apr 17 23:00 file.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh fruits.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим файл `word_count.py` с помощью magic в Jupyter Notebook `%%file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем определит кол-во мапперов и редьюсеров, на пример $10$ мапперов и $3$ редьюсеров. (игрушечный пример)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATAFILE=fruits.txt\n",
    "STREAMING_JAR=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "N=10\n",
    "\n",
    "# N map tasks\n",
    "python word_count.py\\\n",
    "    --jobconf mapreduce.job.maps=$N\\\n",
    "    --jobconf mapreduce.job.reduces=3\\\n",
    "    -r hadoop\\\n",
    "    --hadoop-streaming-jar $STREAMING_JAR\\\n",
    "    $DATAFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "START=$(date +%s);\n",
    "\n",
    "DATAFILE=# файл с большим набором данных\n",
    "STREAMING_JAR=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "N=4 # кол-во мапперов\n",
    "\n",
    "# N map tasks\n",
    "python word_count.py\\\n",
    "    --jobconf mapreduce.job.maps=$N\\\n",
    "    --jobconf mapreduce.job.reduces=3\\\n",
    "    -r hadoop --hadoop-streaming-jar $STREAMING_JAR\\\n",
    "    $DATAFILE\n",
    "    \n",
    "2>/dev/null\n",
    "\n",
    "END=$(date +%s);\n",
    "echo $((END-START)) | awk '{print \"Duration: \"int($1/60)\":\"int($1%60)}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
