{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "first-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string, json\n",
    "\n",
    "import pyspark as ps\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [] \n",
    "    \n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word \\\n",
    "            not in nltk.corpus.stopwords.words('english') \\\n",
    "            and word not in string.punctuation \\\n",
    "            and word != '``':    \n",
    "                tokens.append(word)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recovered-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"best_one\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pending-today",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"essay\":\"\\\\\"I am currently a Special Education Math teacher in a high needs middle school. My students are eager to learn but lack proper classroom resources. I am requesting necessary classroom basics in order to create a supply center. \\\\r\\\\\\\\n\\\\r\\\\\\\\nMy students are very motivated and try their best despite the academic disabilities. In addition to learning the language, my students come from low income homes. All of the students in my school are eligible for free or reduced lunch due to the household incomes. \\\\r\\\\\\\\n\\\\r\\\\\\\\nMy students are classified as emotionally disturbed, academically delayed or learning disabled. They need all the help they can get to get them to the same level as their middle school peers. Most don\\'t have the supplies due to the financial strain buying school supplies puts on the family. Having a large supply of pens, pencils, glue sticks, stapler, and other classroom basics, would be a blessing each day. Most of these materials are needed for everyday lessons, while others like the glue sticks & the stapler, can assist with hands-on learning and project building. If you assist us in acquiring these materials you would be saving many less fortunate from the humiliation of explaining why they are unprepared. I am also requesting visual aids for our classroom walls so they can reference current and prior math concepts. \\\\r\\\\\\\\n\\\\r\\\\\\\\nIf you think back, one of the best things about the first day of school is having brand new sharpened pencils, new art supplies, and a the desire to learn. With your help, our classroom will be ready to start the new school year off right with these basic, necessary school supplies. \\\\\"\"}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_rdd = spark.sparkContext.textFile('essay_1000.json')\n",
    "row_rdd = essay_rdd.map(lambda x: json.loads(x))\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_rdd = row_rdd.filter(lambda row: row['essay'] and row['essay'] != '') \\\n",
    "                       .map(lambda row: row['essay']) \\\n",
    "                       .map(lambda text: text.replace('\\\\n', '').replace('\\r', '')) \\\n",
    "                       .map(lambda text: tokenize(text))\n",
    "\n",
    "essay_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем TF-IDF\n",
    "hashingTF = HashingTF(numFeatures=50000)\n",
    "tf = hashingTF.transform(tokenized_rdd)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-implementation",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seventh-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(tokenized_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(spark.sparkContext, 'word2vec_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить вектора для слов\n",
    "word_vecs = model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_v = word_vecs['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция doc2vec\n",
    "\n",
    "def doc2vec(document_tup):\n",
    "    doc_vec = np.zeros(100)\n",
    "    tot_words = 0\n",
    "    \n",
    "    for word in document_tup[0]:\n",
    "        try:\n",
    "            weight = document_tup[1][hashingTF.indexOf(word)]\n",
    "            vec = np.array([ v for v in word_vecs[word] ])\n",
    "            tot_words += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        doc_vec += weight * vec\n",
    "        \n",
    "    return doc_vec / float(tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка данных\n",
    "ex = tokenized_rdd.zip(tfidf).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применение\n",
    "doc2vec(ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка всего документа\n",
    "document_vectors = tokenized_rdd.zip(tfidf).collect()\n",
    "d2v = [ doc2vec(doc) for doc in document_vectors ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statistical-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def query(q, docs):\n",
    "    '''\n",
    "    функция похожести (косинусное расстояние)\n",
    "    определяем расстояние между векторами для поиска\n",
    "    \n",
    "    '''\n",
    "    tf_q = idf.transform(hashingTF.transform(tokenize(q)))\n",
    "    q_vec = doc2vec((tokenize(q), tf_q))\n",
    "    similarity = distance.cdist(docs, np.array([q_vec]), 'cosine')\n",
    "    return np.argsort(similarity[:, 0])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример\n",
    "query('field trip to aquarium', d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_rdd.zipWithIndex().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем проект\n",
    "def find_projects(indeces, num):\n",
    "    q = indeces[:num]\n",
    "    return essay_rdd.zipWithIndex().filter(lambda x: x[1] in q).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применение\n",
    "find_projects(query('computers', d2v), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-karen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
