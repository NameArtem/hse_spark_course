{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark SQL Tutorial**\n",
    "\n",
    "\n",
    "Функции, которые будут рассмотрены.\n",
    "\n",
    "Transformations:\n",
    "\n",
    "* `select()`, `filter()`, `distinct()`, `dropDuplicates()`, `orderBy()`, `groupBy()`\n",
    "\n",
    "Actions:\n",
    "* `first()`, `take()`, `count()`, `collect()`, `show()`\n",
    "\n",
    "Кэширование:\n",
    "* `cache()`, `unpersist()`\n",
    "\n",
    "Документация [Spark's PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   [Apache Spark](https://spark.apache.org/) с модулем [PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "\n",
    "В Spark, основная коммуникация происходит между driver и executors. Когда мы используем Spark, то driver обращается на Java Virtual Machine (JVM), ожидая запуска выполнения, до тех пор, пока не будет выполнен action. После чего запускается процесс выполнения на executors.\n",
    "\n",
    "Для запуска Spark SQL, нужно запустить `SQLContext`.  Каждая сессия работает с одним [SparkContext]. После запуска `SparkContext` мы инициируем работу с любым другим контекстом `SQLContext` и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Driver общается с Spark через объекты SparkContext, которые являются \"точкой входа\" для построения задач. Пример,Spark SQL контекст (`sqlContext`) в DataBricks содержит Spark DataFrame и SQL функционал (на кластере, вы можете использовать SparkSession / SQLContext / HiveContex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотреть тип контекста (на DataBricks)\n",
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотреть тип контекста (на Cluster (локальный))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `HiveContext` означает, что вы будет работаете с обхектами, которые поддерживают Hive ([Spark Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sqlcontext)), если вы не имеете Hive на кластере, то у вас нет мета-хранилища данных, которые содержит Hive. `HiveContext` - это надстройка над `SQLContext`, которая содержит \"фичи\" для работы с HiveQL, Hive UDFs и Hive таблицами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext attributes\n",
    "\n",
    "Для просмотра атрибутов контекста, вы можете воспользоваться [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) функцией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список sqlContext\n",
    "dir(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help\n",
    "\n",
    "Помощь по объекту. Функция [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основы работы с DataFrames\n",
    "\n",
    "Следующая структура, после RDD, это [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "Методы работы с Spark DF похожи на Pandas DF, будет легко понять и начать использовать.\n",
    "\n",
    "Принцип работы, как с RDD:\n",
    "- DF  - это не изменяемый объект\n",
    "- Сделайте трансформер (ленивые вычисления)\n",
    "- Для запуска используйте Экшен \n",
    "\n",
    "Сделаем следущие шагиs:\n",
    "* Создадим Python объект из 10,000 int\n",
    "* Создадим Spark DataFram\n",
    "* Применим `map`\n",
    "* Применим `collect`\n",
    "* Применим `count`\n",
    "* Применим `filter` и `collect`\n",
    "* Повторим Lambda\n",
    "* Исследуем оценнку производительности и дебаггинг\n",
    "\n",
    "DataFrame - это объект из `Row`; каждая `Row` - это набор из namedtuple объектов. Можно представлять себе это, как таблицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python collection\n",
    "\n",
    "Используем библиотеку [fake-factory](https://pypi.python.org/pypi/fake-factory/0.5.3) to create a collection of fake person records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Factory\n",
    "fake = Factory.create()\n",
    "fake.seed(4321)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим объект из last_name, first_name, ssn, job, age\n",
    "\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "    name = fake.name().split()\n",
    "    return (name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательная функция для вызова других функций в повторе\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in range(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(repeat(10000, fake_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные распределены \n",
    "\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png\" style=\"width: 900px; float: right; margin: 5px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`createDataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sqlContext.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF с колонками\n",
    "dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('type of dataDF: {0}'.format(type(dataDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обзор схемы\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод `registerDataFrameAsTable()` для регистрации данного DF, как таблицы.\n",
    "Это позволит обращаться к данным, через SQL запросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(dataDF, 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как распределенны данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF.type.Method <- обратите внимание на последовательность! Где вы ещё её видели? \n",
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DataFrames и queries\n",
    "\n",
    "Когда мы используем DataFrames или Spark SQL, мы используем \"под капотом\" _query plan_. Каждая трансформация, до экшена, записывается в него. После экшена происходит следущее:\n",
    "\n",
    "1. Spark Catalyst оптимизирует план запроса (без оптимизации план называется _unoptimized logical query plan_). Оптимизация включает в себя пересмотр последовательностей `filter()` и расчет эффективности операций. Часто происходит переконвертация `Decimal` значений в более эффективный long integer. На выходе оптимизатора -  _optimized logical plan_.\n",
    "\n",
    "2. Оптимизированный план строит _physical_ план. Это план для элементов RDD.\n",
    "\n",
    "3. Catalyst делаетрасчет _cost optimization_.\n",
    "\n",
    "4. DataFrame выполняется, как RDD на основе плана выполнения.\n",
    "\n",
    "План можно получить методом `explain()`\n",
    "\n",
    "Подробно в документации [Deep Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = dataDF.distinct().select('*')\n",
    "newDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформируем dataDF методом select.\n",
    "# Метод Select позволяет не только делать выбор, но и трансформировать колонки (age колонка - 1)\n",
    "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age - 1).alias('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLECT\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png\" style=\"height:700px;float:right\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = subDF.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(subDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png\" style=\"height:700px;float:right\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER и COLLECT\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3f.png\" style=\"height:700px;float:right\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF = subDF.filter(subDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Lambda и User Defined Functions\n",
    "\n",
    "Мы можем создавать свои собственные функции. Это необходимо, когда нет другого выхода, но обработать DF надо.\n",
    "\n",
    "Рассмотрим пример: не будем использовать метод `filter()`, а сделаем самостоятельную функцию с `lambda()` и из неё создадим Spark User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "less_ten = udf(lambda s: s < 10, BooleanType())\n",
    "lambdaDF = subDF.filter(less_ten(subDF.age))\n",
    "lambdaDF.show()\n",
    "lambdaDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "even = udf(lambda s: s % 2 == 0, BooleanType())\n",
    "evenDF = lambdaDF.filter(even(lambdaDF.age))\n",
    "evenDF.show()\n",
    "evenDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions (выбор данных)\n",
    "\n",
    "\n",
    "* [first()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first)\n",
    "* [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first: {0}\\n\".format(filteredDF.first()))\n",
    "\n",
    "print(\"Four of them: {0}\\n\".format(filteredDF.take(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# более приятный вид\n",
    "display(filteredDF.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трансформаторы DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orderBy\n",
    "\n",
    "[`orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) \n",
    "\n",
    "* Pandas-style notation: `filteredDF.age`\n",
    "* Subscript notation: `filteredDF['age']`\n",
    "\n",
    "\n",
    "Пример применения:\n",
    "\n",
    "```\n",
    "dataDF.orderBy(dataDF['age'])  # sort by age in ascending order; returns a new DataFrame\n",
    "dataDF.orderBy(dataDF.last_name.desc()) # sort by last name in descending order\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить 5 самых старых людей в DF\n",
    "display(dataDF.orderBy(dataDF.age.desc()).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать `desc()` и `asc()` методы по колонке `orderBy('age'.desc())` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataDF.orderBy('age').take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct и dropDuplicates\n",
    "\n",
    "[`distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) \n",
    "\n",
    "\n",
    "!Note:  `fake-factory` создает мало дублей (или вообще не создает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(dataDF.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = sqlContext.createDataFrame([(\"Vasya\", 1), (\"Vasya\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Roman\", 5)], ('name', 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(\"Vasya\", 1) будет удалено, но строчки с именем \"Anna\" останутся, так как мы применяем ко всей строке метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`dropDuplicates()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates) как `distinct()`, но позволяет указать конкретные колонки для выполнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(dataDF.dropDuplicates(['first_name', 'last_name']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop\n",
    "\n",
    "[`drop()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.drop('occupation').drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "[`groupBy()`]((http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) \n",
    "\n",
    "`groupBy()` не возвращает DataFrame, она возвращает объект [GroupedData](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData), которые содержит функции, после выполнения их вы получите новый DF:\n",
    "\n",
    "* [count()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count)\n",
    "* [sum()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum)\n",
    "* [max()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max)\n",
    "* [avg()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.groupBy().avg('age').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum age: {0}\".format(dataDF.groupBy().max('age').first()[0]))\n",
    "print(\"Minimum age: {0}\".format(dataDF.groupBy().min('age').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample\n",
    "\n",
    "[`sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) может выполнятся с аргументом `withReplacement`, что означает \"фиксировать или нет значения в сэмле\".  `withReplacement=True` будут возвращаться одни и те же данные\n",
    "\n",
    "`fraction` это процент от общего количества данных (`0.20`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledDF = dataDF.sample(withReplacement=False, fraction=0.10)\n",
    "print(sampledDF.count())\n",
    "sampledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.sample(withReplacement=False, fraction=0.05).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache DataFrames\n",
    "filteredDF.cache()\n",
    "# action\n",
    "print(filteredDF.count())\n",
    "# проверка на кэширование\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpersist \n",
    "\n",
    "`unpersist()` удаляет данные из кэша Spark\n",
    "\n",
    "так же можно применить метод [persist()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разкэшировать объект\n",
    "filteredDF.unpersist()\n",
    "# проверка\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема с поиском ошибок в том, что Spark код не выполняется сразу. Разберем пример, как искать ошибки.\n",
    "\n",
    "Создадим функцию, которая не правильно определяет значение в функции\n",
    "\n",
    "Используем `filter()` метод для фильтрации элементов. Ошибок не появится.\n",
    "\n",
    "`filter()` метод не исполняемый до тех пор, пока не будет action. Используем метод `count()` для запуска вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brokenTen(value):\n",
    "    \"\"\"Не правильная функция\n",
    "\n",
    "    Note:\n",
    "        В `if` условии используется `val`, а не `value`.\n",
    "\n",
    "    Args:\n",
    "        value (int)\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "\n",
    "    Raises:\n",
    "        NameError\n",
    "    \"\"\"\n",
    "    if (val < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "btUDF = udf(brokenTen)\n",
    "brokenDF = subDF.filter(btUDF(subDF.age) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполним код\n",
    "brokenDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск ошибок\n",
    "\n",
    "?????? КАК ИСКАТЬ ????????\n",
    "\n",
    "`NameError: global name 'val' is not defined`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Способы поиска\n",
    "\n",
    "\n",
    "Способ, который стоит использовать во время учебы:\n",
    "```\n",
    "    df2 = df1.transformation1()\n",
    "    df2.action1()\n",
    "    df3 = df2.transformation2()\n",
    "    df3.action2()\n",
    "```\n",
    "\n",
    "Способ более прокаченный: \n",
    "`df.transformation1().transformation2().action()`\n",
    "\n",
    "\n",
    "СТОП! Это же просто запуск выполнения! А где же дебаг? (Ждем занятия 8 =) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример более лучшего исполнения\n",
    "myUDF = udf(lambda v: v < 10)\n",
    "subDF.filter(myUDF(subDF.age) == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пару слов о \"code style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "(dataDF\n",
    " .filter(dataDF.age > 20)\n",
    " .select(concat(dataDF.first_name, lit(' '), dataDF.last_name), dataDF.occupation)\n",
    " .show(truncate=False)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "# Select\n",
    "# Filters\n",
    "# Transformators\n",
    "# Select\n",
    "# Filters\n",
    "# Select\n",
    "# Action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "cs105_lab1a_spark_tutorial",
  "notebookId": 4502614177198081
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
