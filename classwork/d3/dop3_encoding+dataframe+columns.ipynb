{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode columns in csv file\n",
    "\n",
    "Рассмотрим разные способы кодирование данных\n",
    "\n",
    "Пример:\n",
    " - в кодировке __A__ мы хотим видеть `a` с позицией `1`, а `b` с `2`, `c` с `3`\n",
    " - в кодировке __B__ мы хотим видеть `a` с позицией `2`, а `b` с `3`, `c` с `1`\n",
    " \n",
    " `A` трансформируем все колонки \n",
    " \n",
    "| c1| c2 |\n",
    "|-----|-----|\n",
    "| a | a|\n",
    "| b | b|\n",
    "| c | b|\n",
    "\n",
    "Результат\n",
    "\n",
    "| c1_enc| c2_enc |\n",
    "|-----|-----|\n",
    "| 1 | 1|\n",
    "| 2 | 2|\n",
    "| 3 | 2|\n",
    "\n",
    "Если `col1` кодирет `A` и `col2` кодирует `B`, то результат\n",
    "\n",
    "| c1_enc| c2_enc |\n",
    "|-----|-----|\n",
    "| 1 | 2|\n",
    "| 2 | 3|\n",
    "| 3 | 3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session / load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .getOrCreate()\n",
    "        \n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv(\"data-1600cols.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('hive.metastore.warehouse.dir', 'file:/home/jovyan/work/spark-warehouse/'),\n",
       " ('spark.app.name', 'Encode multiple columns'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1566927012283'),\n",
       " ('spark.driver.port', '43689'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '172.17.0.7')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1000\n",
      "Number of columns: 1600\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows: {}\\nNumber of columns: {}'.format(df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# есть пустые?\n",
    "df.where(df.V2.isNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| V1| V2| V3|\n",
      "+---+---+---+\n",
      "|  j|  n|  d|\n",
      "|  d|  n|  w|\n",
      "|  p|  h|  a|\n",
      "|  b|  h|  e|\n",
      "|  z|  x|  u|\n",
      "|  b|  e|  v|\n",
      "|  y|  t|  x|\n",
      "|  i|  r|  e|\n",
      "|  x|  e|  g|\n",
      "|  l|  j|  z|\n",
      "|  l|  v|  l|\n",
      "|  z|  n|  h|\n",
      "|  s|  m|  c|\n",
      "|  g|  m|  f|\n",
      "|  i|  p|  n|\n",
      "|  i|  f|  b|\n",
      "|  u|  n|  j|\n",
      "|  s|  o|  e|\n",
      "|  k|  y|  c|\n",
      "|  h|  b|  i|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select.show\n",
    "df.select('V1','V2','V3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первый способ\n",
    "\n",
    "Используем функцию `translate` из `pyspark.sql`, применим её при создании новой колонки `withColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  a|\n",
      "|  b|  b|\n",
      "|  c|  b|\n",
      "+---+---+\n",
      "\n",
      "+---+---+------+------+\n",
      "| c1| c2|c1_enc|c2_enc|\n",
      "+---+---+------+------+\n",
      "|  a|  a|     1|     1|\n",
      "|  b|  b|     2|     2|\n",
      "|  c|  b|     3|     2|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "test_df = sqlContext.createDataFrame([('a', 'a'), ('b', 'b'), ('c', 'b')], ['c1', 'c2'])\n",
    "test_df.show()\n",
    "\n",
    "chars = \"abc\"\n",
    "A = \"123\" # encoding A\n",
    "B = \"231\" # encoding B\n",
    "\n",
    "\n",
    "for col_name in [\"c1\", \"c2\"]:  \n",
    "    test_df = test_df.withColumn(col_name+'_enc', f.translate(f.col(col_name), \"abcd\", A))\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличим размености и число вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings:\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "84909340662170830129865816\n",
      "03946914819742444812351068\n",
      "--------------------------\n",
      "+---+---+---+---+------+------+------+------+\n",
      "| V1| V2| V3| V4|V1_enc|V2_enc|V3_enc|V4_enc|\n",
      "+---+---+---+---+------+------+------+------+\n",
      "|  j|  n|  d|  m|     6|     2|     0|     4|\n",
      "|  d|  n|  w|  y|     0|     2|     5|     6|\n",
      "|  p|  h|  a|  h|     3|     4|     8|     4|\n",
      "|  b|  h|  e|  t|     4|     4|     9|     2|\n",
      "|  z|  x|  u|  d|     6|     0|     8|     4|\n",
      "|  b|  e|  v|  j|     4|     6|     6|     1|\n",
      "|  y|  t|  x|  w|     1|     2|     8|     1|\n",
      "|  i|  r|  e|  q|     6|     8|     9|     4|\n",
      "|  x|  e|  g|  s|     8|     6|     4|     1|\n",
      "|  l|  j|  z|  h|     1|     1|     6|     4|\n",
      "|  l|  v|  l|  w|     1|     5|     1|     1|\n",
      "|  z|  n|  h|  z|     6|     2|     0|     8|\n",
      "|  s|  m|  c|  z|     2|     4|     9|     8|\n",
      "|  g|  m|  f|  j|     4|     4|     3|     1|\n",
      "|  i|  p|  n|  h|     6|     4|     0|     4|\n",
      "|  i|  f|  b|  r|     6|     9|     4|     8|\n",
      "|  u|  n|  j|  p|     8|     2|     6|     4|\n",
      "|  s|  o|  e|  f|     2|     4|     9|     9|\n",
      "|  k|  y|  c|  c|     2|     6|     9|     9|\n",
      "|  h|  b|  i|  p|     0|     3|     6|     4|\n",
      "+---+---+---+---+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "# seed\n",
    "random.seed(30)\n",
    "\n",
    "chars = string.ascii_lowercase\n",
    "encodingA = ''.join(random.choice(string.digits) for i in range(len(chars)))\n",
    "encodingB = ''.join(random.choice(string.digits) for i in range(len(chars)))\n",
    "\n",
    "print(\"Encodings:\")\n",
    "print(chars)\n",
    "print(encodingA)\n",
    "print(encodingB)\n",
    "print(\"-\"*26)\n",
    "new_df=df\n",
    "\n",
    "for col_name in [\"V1\", \"V3\"]:  # apply encodingA to columns V1, V3\n",
    "    new_df=new_df.withColumn(col_name+'_enc',f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in [\"V2\", \"V4\"]:  # apply encodingB to columns V2, V4\n",
    "    new_df=new_df.withColumn(col_name+'_enc',f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select(\"V1\",\"V2\",\"V3\",\"V4\", \"V1_enc\", \"V2_enc\", \"V3_enc\", \"V4_enc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именование колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| V1| V2| V3| V4|\n",
      "+---+---+---+---+\n",
      "|  6|  2|  0|  4|\n",
      "|  0|  2|  5|  6|\n",
      "|  3|  4|  8|  4|\n",
      "+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df=df\n",
    "\n",
    "for col_name in [\"V1\", \"V3\"]:  # apply encodingA to columns V1, V2\n",
    "    new_df = new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in [\"V2\", \"V4\"]:  # apply encodingB to columns V3, V4\n",
    "    new_df = new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select(\"V1\",\"V2\",\"V3\",\"V4\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V2', 'V4']\n",
      "['V1', 'V3']\n",
      "+---+---+---+---+\n",
      "| V1| V2| V3| V4|\n",
      "+---+---+---+---+\n",
      "|  6|  2|  0|  4|\n",
      "|  0|  2|  5|  6|\n",
      "|  3|  4|  8|  4|\n",
      "+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_e = [\"V\"+str(i) for i in range(2,5,2)]\n",
    "cols_o = [\"V\"+str(i) for i in range(1,4,2)]\n",
    "\n",
    "print(cols_e)\n",
    "print(cols_o)\n",
    "\n",
    "new_df=df\n",
    "\n",
    "for col_name in cols_o:  # apply encodingA to columns with even numbers\n",
    "    new_df=new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in cols_e:  # apply encodingB to odd columns \n",
    "    new_df=new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select([\"V\"+str(i) for i in range(1,5)]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Второй способ\n",
    "Напишем `udf` (user-defined functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1', 'V3', 'V5']\n",
      "+---+---+---+------+------+------+\n",
      "| V1| V3| V5|V1_enc|V3_enc|V5_enc|\n",
      "+---+---+---+------+------+------+\n",
      "|  j|  d|  s|     6|     0|     2|\n",
      "|  d|  w|  l|     0|     5|     1|\n",
      "|  p|  a|  w|     3|     8|     5|\n",
      "|  b|  e|  x|     4|     9|     8|\n",
      "|  z|  u|  b|     6|     8|     4|\n",
      "|  b|  v|  u|     4|     6|     8|\n",
      "|  y|  x|  z|     1|     8|     6|\n",
      "|  i|  e|  k|     6|     9|     2|\n",
      "|  x|  g|  s|     8|     4|     2|\n",
      "|  l|  z|  l|     1|     6|     1|\n",
      "+---+---+---+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# определим кодировку\n",
    "o = [\"abcdefghijklmnopqrstuvwxyz\", encodingA]\n",
    "\n",
    "def enc(*a):\n",
    "    # перекодируем\n",
    "    s=a[0]\n",
    "    for i in range(len(o[0])): \n",
    "        if s==o[0][i]:      \n",
    "            return o[1][i]\n",
    "    return s\n",
    "\n",
    "# создадим udf\n",
    "encode_udf = udf(enc, StringType())\n",
    "\n",
    "cols_o = [\"V\"+str(i) for i in range(7) if i%2==1]\n",
    "print(cols_o)\n",
    "\n",
    "(\n",
    "df.select(\"V1\",\"V3\",\"V5\", \n",
    "           encode_udf(\"V1\").alias(\"V1_enc\"),\n",
    "           encode_udf(\"V3\").alias(\"V3_enc\"),\n",
    "           encode_udf(\"V5\").alias(\"V5_enc\"))\n",
    "    .show(10) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|V1_enc|V3_enc|V5_enc|V7_enc|V9_enc|V11_enc|V13_enc|V15_enc|V17_enc|V19_enc|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|     6|     0|     2|     6|     9|      8|      2|      2|      3|      6|\n",
      "|     0|     5|     1|     8|     0|      2|      9|      6|      8|      2|\n",
      "|     3|     8|     5|     4|     8|      3|      9|      0|      2|      9|\n",
      "|     4|     9|     8|     0|     9|      0|      9|      2|      8|      0|\n",
      "|     6|     8|     4|     0|     9|      2|      8|      6|      6|      6|\n",
      "|     4|     6|     8|     5|     8|      6|      5|      6|      6|      4|\n",
      "|     1|     8|     6|     0|     4|      8|      4|      5|      5|      1|\n",
      "|     6|     9|     2|     5|     8|      8|      5|      4|      0|      1|\n",
      "|     8|     4|     2|     2|     2|      2|      9|      7|      8|      0|\n",
      "|     1|     6|     1|     9|     6|      2|      6|      1|      1|      2|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применим в select перебор колонок через цикл\n",
    "new_df=df.select([encode_udf(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,100,2)])\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(1,21,2)]).show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
