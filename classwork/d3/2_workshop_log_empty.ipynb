{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import lit, concat\n",
    "from pyspark.sql import functions as sqlFunctions\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import dayofmonth\n",
    "\n",
    "#from spark_notebook_helpers import prepareSubplot, np, plt, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования RE\n",
    "# m = re.search('(?<=abc)def', 'abcdef')\n",
    "# m.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Используем данные логов от NASA Kennedy Space Center и проведем анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "Создадим `sqlContext` и прочитаем данные `sqlContext.read.text()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = sqlContext.read.text(file_path)\n",
    "base_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы представлены в формате [Common Log Format](https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format)\n",
    "\n",
    "_remotehost rfc931 authuser [date] \"request\" status bytes_\n",
    "\n",
    "| field         | meaning                                                                |\n",
    "| ------------- | ---------------------------------------------------------------------- |\n",
    "| _remotehost_  | Remote hostname (or IP number if DNS hostname is not available).       |\n",
    "| _rfc931_      | The remote logname of the user. We don't really care about this field. |\n",
    "| _authuser_    | The username of the remote user, as authenticated by the HTTP server.  |\n",
    "| _[date]_      | The date and time of the request.                                      |\n",
    "| _\"request\"_   | The request, exactly as it came from the browser or client.            |\n",
    "| _status_      | The HTTP status code the server sent back to the client.               |\n",
    "| _bytes_       | The number of bytes (`Content-Length`) transferred to the client.      |\n",
    "\n",
    "\n",
    "Используем [regexp\\_extract()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract) для извлечения данных.\n",
    "\n",
    "Если вы хотите познакомиться с Regular Expressions ближе, то рекомендую книгу [_Regular Expressions Cookbook_](http://shop.oreilly.com/product/0636920023630.do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, regexp_extract\n",
    "split_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n",
    "                          regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('timestamp'),\n",
    "                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n",
    "                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n",
    "                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почистим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# счет пропусков\n",
    "base_df.filter(base_df['value'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если парсинг правильный, то пустых не должно быть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rows_df = split_df.filter(split_df['host'].isNull() |\n",
    "                              split_df['timestamp'].isNull() |\n",
    "                              split_df['path'].isNull() |\n",
    "                              split_df['status'].isNull() |\n",
    "                             split_df['content_size'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчет по каждой колонке\n",
    "def count_null(col_name):\n",
    "    return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "\n",
    "exprs = []\n",
    "for col_name in split_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "\n",
    "# сделаем агрегат по содержанию в exprs\n",
    "split_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-во не подходящего контента\n",
    "bad_content_size_df = base_df.filter(~ base_df['value'].rlike(r'\\d+$'))\n",
    "bad_content_size_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заглянем в нутрь плохих строк\n",
    "bad_content_size_df.select(concat(bad_content_size_df['value'], lit('*'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исправим плохие строки\n",
    "\n",
    "Воспользуемся \n",
    "* [fillna()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna)\n",
    "* [na](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na) возвращает [DataFrameNaFunctions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменим все content_size на 0.\n",
    "cleaned_df = split_df.na.fill({'content_size': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим, что пустот больше нет\n",
    "exprs = []\n",
    "for col_name in cleaned_df.columns:\n",
    "    exprs.append(count_null(col_name))\n",
    "\n",
    "cleaned_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг TimeStamp\n",
    "\n",
    "Создадим UDF для парсинга\n",
    "\n",
    "А какие способы обработки временных данных вы знаете? (для моделей машинного обучения?)\n",
    "\n",
    "Вот вам примеры:\n",
    "\n",
    "- [stackexchange](https://stats.stackexchange.com/questions/126230/optimal-construction-of-day-feature-in-neural-networks)\n",
    "- [kaggle](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n",
    "- [stackexchange another](https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes)\n",
    "- [github](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/)\n",
    "- [medium](https://medium.com/ai%C2%B3-theory-practice-business/top-6-errors-novice-machine-learning-engineers-make-e82273d394db)\n",
    "- [towardsdatascience](https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {\n",
    "  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "def parse_clf_time(s):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        s (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n",
    "    Returns:\n",
    "        a string  to CAST('timestamp')\n",
    "    \"\"\"\n",
    "    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n",
    "      int(s[7:11]),\n",
    "      month_map[s[3:6]],\n",
    "      int(s[0:2]),\n",
    "      int(s[12:14]),\n",
    "      int(s[15:17]),\n",
    "      int(s[18:20])\n",
    "    )\n",
    "\n",
    "u_parse_time = udf(parse_clf_time)\n",
    "\n",
    "logs_df = cleaned_df.select('*', u_parse_time(cleaned_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\n",
    "total_log_entries = logs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Базовый анализ\n",
    "\n",
    "`.describe()` для получения: count, mean, stddev, min и max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_size_summary_df = logs_df.describe(['content_size'])\n",
    "content_size_summary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или применим что-нибудь из набора функций `pyspark.sql.functions` - [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получите минимальное, среднее и максимальное значение по content_size\n",
    "content_size_stats =  (logs_df\n",
    "                       #\n",
    "                       # ваш код здесь\n",
    "                       #\n",
    "                       .first())\n",
    "\n",
    "print('Using SQL functions:')\n",
    "print('Content Size Avg: {1:,.2f}; Min: {0:.2f}; Max: {2:,.0f}'.format(*content_size_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Status \n",
    "\n",
    "Сделаем обзор данных по `status` и применим сортировку для сгруппированного объекта по типу `status`\n",
    "! Не забудем сделать `cache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получите кол-во элементов по каждому status и отсортируйте по status\n",
    "status_to_count_df =(logs_df\n",
    "                        #\n",
    "                        # ваш код здесь\n",
    "                        #\n",
    "                    )\n",
    "                     \n",
    "                     \n",
    "                     \n",
    "status_to_count_length = status_to_count_df.count()\n",
    "print('Found %d response codes' % status_to_count_length)\n",
    "status_to_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация\n",
    "\n",
    "Для визуализации используйте `display()` \n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/plot_options_1.png\" style=\"float: right; margin-right: 30px; border: 1px solid #999999\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(status_to_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из-за большой разницы в количестве, нет хорошего отображения данных\n",
    "# что можно применить, чтобы исправить визализацию?\n",
    "log_status_to_count_df = status_to_count_df.withColumn('log(count)', sqlFunctions.log(status_to_count_df['count']))\n",
    "\n",
    "display(log_status_to_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим ещё одну библиотеку в нашу работу [`spark_notebook_helpers`](https://pypi.python.org/pypi/spark_notebook_helpers/1.0.1), она помогает визуализировать и узнавать объекты Spark\n",
    "\n",
    "![](notebook_helper.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример\n",
    "help(prepareSubplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации используем matplotlib и набор \"Set1\". Другие наборы на [сайте](http://matplotlib.org/examples/color/colormaps_reference.html). Разные схемы цветов позволяют сделать разные акценты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = log_status_to_count_df.drop('count').collect()\n",
    "# x, y = zip(*data)\n",
    "# index = np.arange(len(x))\n",
    "# bar_width = 0.7\n",
    "# colorMap = 'Set1'\n",
    "# cmap = cm.get_cmap(colorMap)\n",
    "\n",
    "# fig, ax = prepareSubplot(np.arange(0, 6, 1), np.arange(0, 14, 2))\n",
    "# plt.bar(index, y, width=bar_width, color=cmap(0))\n",
    "# plt.xticks(index + bar_width/2.0, x)\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определить частоту встречамости Host\n",
    "\n",
    "Создадим новый DF, который будет включать агрегаты по `host` и их количество.\n",
    "Сделаем фильтр по количеству, чтобы в DF остались хосты, которые встречаются больше 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_sum_df =(logs_df\n",
    "              .groupBy('host')\n",
    "              .count())\n",
    "\n",
    "host_more_than_10_df = (host_sum_df\n",
    "                        .filter(host_sum_df['count'] > 10)\n",
    "                        .select(host_sum_df['host']))\n",
    "\n",
    "print('Any 20 hosts that have accessed more then 10 times:\\n')\n",
    "host_more_than_10_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация Path\n",
    "\n",
    "Сделаем группированный по `path` DF, который будет сортированный по убыванию количества встречания `path` \n",
    "\n",
    "Для визуазиции данных, мы извлечем `path` и `count` из DF в объект PairRDD (к каждой `Rows` применить функцию `lambda` через `map`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_df = (logs_df\n",
    "            .groupBy('path')\n",
    "            .count()\n",
    "            .sort('count', ascending=False)\n",
    "           )\n",
    "\n",
    "paths_counts = (paths_df\n",
    "                .select('path', 'count')\n",
    "                .map(lambda r: (r[0], r[1]))\n",
    "                .collect())\n",
    "\n",
    "paths, counts = zip(*paths_counts)\n",
    "\n",
    "# colorMap = 'Accent'\n",
    "# cmap = cm.get_cmap(colorMap)\n",
    "# index = np.arange(1000)\n",
    "\n",
    "# fig, ax = prepareSubplot(np.arange(0, 1000, 100), np.arange(0, 70000, 10000))\n",
    "# plt.xlabel('Paths')\n",
    "# plt.ylabel('Number of Hits')\n",
    "# plt.plot(index, counts[:1000], color=cmap(0), linewidth=3)\n",
    "# plt.axhline(linewidth=2, color='#999999')\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(paths_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Paths\n",
    "\n",
    "Используя метод `.show()` извлечем ТОП `n=10` и отобразим `truncate=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Paths\n",
    "print('Top Ten Paths:')\n",
    "paths_df.show(n=10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналитика по Log File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top Ten Error Paths**\n",
    "\n",
    "Код нормальной работы в web - `200`. А сколько было сделано запросов с другими кодами?\n",
    "\n",
    "Сделайте DF, который покажет, сколько было запросов, которые вернули нет 200 код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтр по DataFrame, чтобы убрать все 200\n",
    "not200DF = logs_df.filter(# ваш код здесь)\n",
    "not200DF.show(10)\n",
    "\n",
    "# Сделаем группировку по path и сортировку по убыванию (по количеству)\n",
    "logs_sum_df = (not200DF\n",
    "               .groupby('path')\n",
    "               .count()\n",
    "               .sort(desc('count'))\n",
    "               .cache())\n",
    "\n",
    "print()'Top Ten failed URLs:')\n",
    "logs_sum_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем PairRDD по Топ 10 ошибок\n",
    "top_10_err_urls = [(row[0], row[1]) for row in logs_sum_df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество уникальных Host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# найдем уникальные Host и посчитаем их количество\n",
    "unique_host_count = (logs_df\n",
    "                     #\n",
    "                     # ваш код здесь\n",
    "                     #\n",
    "                    ).count()\n",
    "\n",
    "\n",
    "print('Unique hosts: {0}'.format(unique_host_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество уникальных  Host в день\n",
    "\n",
    "Используя функцию [`dayofmonth` function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth) из `pyspark.sql.functions` модуля, найдите уникальное количество host на каждый день.\n",
    "\n",
    "\n",
    "**`day_to_host_pair_df`**\n",
    "\n",
    "DataFrame должен содержать следующие колонки\n",
    "\n",
    "| column | explanation          |\n",
    "| ------ | -------------------- |\n",
    "| `host` | имя host             |\n",
    "| `day`  | день в месяце        |\n",
    "\n",
    "\n",
    "В каждой строке `logs_df` содержится дата и время:\n",
    "\n",
    "```\n",
    "gw1.att.com - - [23/Aug/1995:00:03:53 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -\n",
    "```\n",
    "\n",
    "Из предыдущей строки можно извлечь для `day_to_host_pair_df`:\n",
    "\n",
    "```\n",
    "gw1.att.com 23\n",
    "\n",
    "[23/Aug/1995:00:03:53 -0400]\n",
    "```\n",
    "\n",
    "**`day_group_hosts_df`**\n",
    "\n",
    "DataFrame `day_to_host_pair_df` содержит сгруппированные данные по (`day`, `host`), без дубликатов.\n",
    "\n",
    "**`daily_hosts_df`**\n",
    "\n",
    "Результатом должен быть DataFrame\n",
    "\n",
    "| column  | explanation                                        |\n",
    "| ------- | -------------------------------------------------- |\n",
    "| `day`   | день месяца                                        |\n",
    "| `count` | кол-во уникальных хостов                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_to_host_pair_df = logs_df.select(logs_df.host,\n",
    "                                     dayofmonth('time').alias('day'))\n",
    "\n",
    "day_group_hosts_df = day_to_host_pair_df.distinct()\n",
    "\n",
    "daily_hosts_df = day_group_hosts_df.select(day_group_hosts_df['day']).groupby(day_group_hosts_df['day']).count().cache()\n",
    "\n",
    "print('Unique hosts per day: \\n',daily_hosts_df.show(10))\n",
    "daily_hosts_df.show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация уникальных\n",
    "\n",
    "**WARNING**: Запомним, что `collect()` возвращает список из `Row`, данные надо будет обработать и после `collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список дней\n",
    "days_with_hosts = daily_hosts_df.select(daily_hosts_df['day'])\n",
    "\n",
    "# списко уникальных хостов\n",
    "hosts = daily_hosts_df.select(daily_hosts_df['count'])\n",
    "\n",
    "days_with_hosts, hosts = [list(i) for i in zip(*daily_hosts_df.select('day', 'count').map(lambda r: (r[0], r[1])).collect())]\n",
    "print(days_with_hosts)\n",
    "print(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = prepareSubplot(np.arange(0, 30, 5), np.arange(0, 5000, 1000))\n",
    "# colorMap = 'Dark2'\n",
    "# cmap = cm.get_cmap(colorMap)\n",
    "# plt.plot(days_with_hosts, hosts, color=cmap(0), linewidth=3)\n",
    "# plt.axis([0, max(days_with_hosts), 0, max(hosts)+500])\n",
    "# plt.xlabel('Day')\n",
    "# plt.ylabel('Hosts')\n",
    "# plt.axhline(linewidth=3, color='#999999')\n",
    "# plt.axvline(linewidth=2, color='#999999')\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(daily_hosts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среднее по запросу в день на каждый Host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расчет среднего по хосту в день = (все запросы в день)/(количество уникальных хостов в день)\n",
    "\n",
    "# всего запросов в день\n",
    "total_req_per_day_df = logs_df.select(dayofmonth('time').alias('day')).groupby('day').count()\n",
    "\n",
    "print(total_req_per_day_df.show(10))\n",
    "\n",
    "\n",
    "\n",
    "# расчет среднего\n",
    "avg_daily_req_per_host_df = (\n",
    "  total_req_per_day_df.join( daily_hosts_df, daily_hosts_df['day'] == total_req_per_day_df['day'])\n",
    "   .drop(daily_hosts_df['day'])\n",
    "   .select(\n",
    "    total_req_per_day_df['day'], (total_req_per_day_df['count'] / daily_hosts_df['count']).alias('avg_reqs_per_host_per_day')\n",
    "  )\n",
    ").cache()\n",
    "\n",
    "print('Average number of daily requests per Hosts is:\\n')\n",
    "avg_daily_req_per_host_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Exploring 404 Status Codes\n",
    "\n",
    "Let's drill down and explore the error 404 status records. We've all seen those \"404 Not Found\" web pages. 404 errors are returned when the server cannot find the resource (page or object) the browser or client requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5a) Exercise: Counting 404 Response Codes\n",
    "\n",
    "Create a DataFrame containing only log records with a 404 status code. Make sure you `cache()` `not_found_df` as we will use it in the rest of this exercise.\n",
    "\n",
    "How many 404 records are in the log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "not_found_df = logs_df.filter(logs_df['status'] == '404').cache()\n",
    "print('Found {0} 404 URLs').format(not_found_df.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Counting 404 (5a)\n",
    "Test.assertEquals(not_found_df.count(), 6185, 'incorrect not_found_df.count()')\n",
    "Test.assertTrue(not_found_df.is_cached, 'incorrect not_found_df.is_cached')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исследование 404\n",
    "\n",
    "Сделаем исследование ошибки 404 в логах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор только 404\n",
    "not_found_paths_df = not_found_df.select('path').cache()\n",
    "# только уникальные пути с ошибкой 404\n",
    "unique_not_found_paths_df = not_found_paths_df.distinct()\n",
    "\n",
    "print('404 URLS:\\n')\n",
    "unique_not_found_paths_df.show(n=40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 путей по ошибке 404\n",
    "top_20_not_found_df = not_found_paths_df.groupby('path').count().sort(desc('count')).cache()\n",
    "\n",
    "print('Top Twenty 404 URLs:\\n')\n",
    "top_20_not_found_df.show(n=20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решите самостоятельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# топ 25 Host, которые создают ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# топ 5 дней с ошибками 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ошибки 404 по часам"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "cs105_lab2_apache_log",
  "notebookId": 855429340486486
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
